{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 52, 62, 46, 49, 47, 43, 49, 50, 44, 49, 39, 184, 113, 132, 94, 147, 208, 87, 143, 83, 136, 79, 105] 2088\n",
      "2.2509578544061304\n",
      "2.490421455938697\n",
      "2.9693486590038316\n",
      "2.203065134099617\n",
      "2.346743295019157\n",
      "2.2509578544061304\n",
      "2.0593869731800765\n",
      "2.346743295019157\n",
      "2.3946360153256707\n",
      "2.10727969348659\n",
      "2.346743295019157\n",
      "1.8678160919540232\n",
      "8.812260536398467\n",
      "5.411877394636015\n",
      "6.321839080459771\n",
      "4.501915708812261\n",
      "7.040229885057471\n",
      "9.961685823754788\n",
      "4.166666666666666\n",
      "6.8486590038314175\n",
      "3.9750957854406135\n",
      "6.513409961685824\n",
      "3.7835249042145596\n",
      "5.028735632183908\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "training_data = []\n",
    "\n",
    "class Labeling():\n",
    "    \n",
    "    #Labels = {C_major : 0 , Cs_major : 1, D_major : 2, Eb_major : 3, E_major : 4, F_major : 5, Fs_major : 6, G_major : 7, Ab_major : 8, A_major : 9, Bb_major : 10, B_major : 11, \n",
    "    #          C_major : 12 , C_minor : 13, D_minor : 14, Eb_minor : 15, E_minor : 16, F_minor : 17, Fs_minor : 18, G_minor : 19, Ab_minor : 20, A_minor : 21, Bb_minor : 22, B_minor : 23\n",
    "    #         }\n",
    "    training_data = []\n",
    "    \n",
    "    \n",
    "    def make_training_data():\n",
    "        path = \"C:\\spects\"\n",
    "        #path = \"C:/outs/spects8192\"\n",
    "        os.chdir(path)\n",
    "        spectrograms = os.listdir()\n",
    "        key_count = [0] * 24\n",
    "        annotations = open(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/Annotations2090.txt\", \"r\")\n",
    "        count = 0\n",
    "        \n",
    "        while True: \n",
    "            line = annotations.readline()\n",
    "            if not line:\n",
    "                print(key_count , count)\n",
    "                for keys in key_count :\n",
    "                    print(((keys/len(training_data)))*100)\n",
    "                break;\n",
    "            \n",
    "            count = count + 1\n",
    "            file, key = line.split(' ', 1)\n",
    "            key = key.strip()\n",
    "            #print(key)\n",
    "            file = file + \".LOFI.png\"\n",
    "            spect_path = os.path.join(path, file)\n",
    "            #spect_path = cv2.imread(spect_path)\n",
    "            spect_path = cv2.imread(spect_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if spect_path is None : \n",
    "                continue;\n",
    "            \n",
    "            \n",
    "            if key == 'C major' :\n",
    "                if key_count[0] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([0])])\n",
    "                key_count[0] = key_count[0] + 1\n",
    "            elif key == 'C# major' :\n",
    "                if key_count[1] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([1])])   #|\n",
    "                key_count[1] = key_count[1] + 1                                       #|\n",
    "            elif key == 'Db major' :\n",
    "                if key_count[1] >600:\n",
    "                    continue;                                                         #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([1])])   #|\n",
    "                key_count[1] = key_count[1] + 1                                       #|\n",
    "            elif key == 'D major' :\n",
    "                if key_count[2] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([2])])\n",
    "                key_count[2] = key_count[2] + 1\n",
    "            elif key == 'D# major' :\n",
    "                if key_count[3] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([3])])  #|\n",
    "                key_count[3] = key_count[3] + 1\n",
    "            elif key == 'Eb major' :\n",
    "                if key_count[3] >600:\n",
    "                    continue;                                                        #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([3])])  #|\n",
    "                key_count[3] = key_count[3] + 1\n",
    "            elif key == 'E major' :\n",
    "                if key_count[4] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([4])])\n",
    "                key_count[4] = key_count[4] + 1\n",
    "            elif key == 'F major' :\n",
    "                if key_count[5] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([5])])\n",
    "                key_count[5] = key_count[5] + 1\n",
    "            elif key == 'F# major' :\n",
    "                if key_count[6] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([6])])  #|\n",
    "                key_count[6] = key_count[6] + 1\n",
    "            elif key == 'Gb major' :      \n",
    "                if key_count[6] >600:\n",
    "                    continue;                                                        #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([6])])  #|\n",
    "                key_count[6] = key_count[6] + 1\n",
    "            elif key == 'G major' :\n",
    "                if key_count[7] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([7])])\n",
    "                key_count[7] = key_count[7] + 1\n",
    "            elif key == 'G# major' :\n",
    "                if key_count[8] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path),torch.LongTensor([8])])  #|\n",
    "                key_count[8] = key_count[8] + 1\n",
    "            elif key == 'Ab major' :   \n",
    "                if key_count[8] >600:\n",
    "                    continue;                                                       #| same key\n",
    "                training_data.append([np.array(spect_path),torch.LongTensor([8])])  #|\n",
    "                key_count[8] = key_count[8] + 1\n",
    "            elif key == 'A major' :\n",
    "                if key_count[9] > 600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([9])])\n",
    "                key_count[9] = key_count[9] + 1\n",
    "            elif key == 'A# major' :\n",
    "                if key_count[10] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path),torch.LongTensor([10])]) #|\n",
    "                key_count[10] = key_count[10] + 1\n",
    "            elif key == 'Bb major' :    \n",
    "                if key_count[10] >600:\n",
    "                    continue;                                                        #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([10])]) #|\n",
    "                key_count[10] = key_count[10] + 1\n",
    "            elif key == 'B major' :\n",
    "                if key_count[11] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([11])])\n",
    "                key_count[11] = key_count[11] + 1\n",
    "            elif key == 'C minor' :\n",
    "                if key_count[12] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([12])])\n",
    "                key_count[12] = key_count[12] + 1\n",
    "            elif key == 'C# minor' :\n",
    "                if key_count[13] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([13])])   #|\n",
    "                key_count[13] = key_count[13] + 1\n",
    "            elif key == 'Db minor' :\n",
    "                if key_count[13] >600:\n",
    "                    continue;                                                          #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([13])])   #|\n",
    "                key_count[13] = key_count[13] + 1\n",
    "            elif key == 'D minor' :\n",
    "                if key_count[14] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([14])])\n",
    "                key_count[14] = key_count[14] + 1\n",
    "            elif key == 'D# minor' :\n",
    "                if key_count[15] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([15])])  #|\n",
    "                key_count[15] = key_count[15] + 1\n",
    "            elif key == 'Eb minor' :\n",
    "                if key_count[15] >600:\n",
    "                    continue;                                                         #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([15])])  #|\n",
    "                key_count[15] = key_count[15] + 1\n",
    "            elif key == 'E minor' :\n",
    "                if key_count[16] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([16])])\n",
    "                key_count[16] = key_count[16] + 1\n",
    "            elif key == 'F minor' :\n",
    "                if key_count[17] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path),torch.LongTensor([17])])\n",
    "                key_count[17] = key_count[17] + 1\n",
    "            elif key == 'F# minor' :\n",
    "                if key_count[18] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([18])])  #|\n",
    "                key_count[18] = key_count[18] + 1\n",
    "            elif key == 'Gb minor' :\n",
    "                if key_count[18] >600:\n",
    "                    continue;                                                         #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([18])])  #|\n",
    "                key_count[18] = key_count[18] + 1\n",
    "            elif key == 'G minor' :\n",
    "                if key_count[19] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([19])])\n",
    "                key_count[19] = key_count[19] + 1\n",
    "            elif key == 'G# minor' :\n",
    "                if key_count[20] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([20])])  #|\n",
    "                key_count[20] = key_count[20] + 1\n",
    "            elif key == 'Ab minor' :\n",
    "                if key_count[20] >600:\n",
    "                    continue;                                                         #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([20])])  #|\n",
    "                key_count[20] = key_count[20] + 1\n",
    "            elif key == 'A minor' :\n",
    "                if key_count[21] >600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([21])])\n",
    "                key_count[21] = key_count[21] + 1\n",
    "            elif key == 'A# minor' :\n",
    "                if key_count[22] > 600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([22])]) #|\n",
    "                key_count[22] = key_count[22] + 1\n",
    "            elif key == 'Bb minor' :\n",
    "                if key_count[22] > 600:\n",
    "                    continue;                                                        #| same key\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([22])]) #|\n",
    "                key_count[22] = key_count[22] + 1\n",
    "            elif key == 'B minor' :\n",
    "                if key_count[23] > 600:\n",
    "                    continue;\n",
    "                training_data.append([np.array(spect_path), torch.LongTensor([23])])\n",
    "                key_count[23] = key_count[23] + 1\n",
    "\n",
    "        \n",
    "        np.random.shuffle(training_data)\n",
    "        #np.save(\"training_data.npy\", training_data)            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Labeling.make_training_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset trainable parameters of layer = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "Reset trainable parameters of layer = Linear(in_features=454656, out_features=48, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=48, out_features=24, bias=True)\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=454656, out_features=48, bias=True)\n",
      "  (fc2): Linear(in_features=48, out_features=24, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "        #self.conv2 = nn.Conv2d(8, 8, 5)\n",
    "        #self.conv3 = nn.Conv2d(8, 8, 5)\n",
    "        #self.conv4 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 3)\n",
    "        \n",
    "        x = torch.randn(52,300).view(-1,1,52,300)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 48)\n",
    "        self.fc2 = nn.Linear(48, 24)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv1(x)), (2,2))\n",
    "        \n",
    "        #x = F.elu(self.conv2(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv2(x)), (2,2))\n",
    "        \n",
    "        #x = F.elu(self.conv3(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv3(x)), (2,2))\n",
    "        \n",
    "        #x = F.elu(self.conv4(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv4(x)), (2,2))\n",
    "        \n",
    "        x = F.avg_pool2d(F.elu(self.conv5(x)), (2,2))\n",
    "        \n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "            #print(x.shape)\n",
    "            #print(self._to_linear)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "network = Net().to(device)\n",
    "reset_weights(network)\n",
    "print(network)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "#training_data = np.load(\"C:/spectsBS/training_data.npy\", allow_pickle=True)\n",
    "#print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308\n"
     ]
    }
   ],
   "source": [
    "#import sys \n",
    "#import torch\n",
    "\n",
    "#print(sys.getsizeof(training_data[0]))\n",
    "#temp = torch.Tensor([training_data[0]]).view(-1,105,600)\n",
    "#print(temp)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.0005, momentum = 0.9, weight_decay = 0.0001) \n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in range(0, training_data, 32):\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,52,300)\n",
    "\n",
    "\n",
    "#print(\"cap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "18664\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(X[0]))\n",
    "print(sys.getsizeof(training_data))\n",
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2088\n",
      "2088\n"
     ]
    }
   ],
   "source": [
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "1880\n",
      "208\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "\n",
    "print(val_size)\n",
    "train_X = X[:len(training_data)-val_size]\n",
    "train_y = y[:len(training_data)-val_size]\n",
    "\n",
    "print(len(train_X))\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(test_X))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 15\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(net):\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 52, 300)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "            \n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            network.zero_grad()\n",
    "\n",
    "            outputs = network(batch_X)\n",
    "            \n",
    "            #print(outputs)\n",
    "            #print(outputs)\n",
    "            batch_y = batch_y.long()\n",
    "            #print(batch_y.shape)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:40<00:00,  3.12it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.028388654813170433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:32<00:00,  3.92it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.022655632346868515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:32<00:00,  3.91it/s]\n",
      "  2%|▏         | 2/126 [00:00<00:10, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.02779572084546089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:23<00:00,  5.41it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.025853430852293968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:25<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.017228757962584496\n"
     ]
    }
   ],
   "source": [
    "train(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 353.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  7.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "network.eval()\n",
    "test_X , test_y = test_X.to(device), test_y.to(device)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        #real_class = torch.argmax(test_y[i])\n",
    "        net_out = network(test_X[i].view(-1, 1, 52, 300))[0]  # returns a list, \n",
    "        \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        \n",
    "        \n",
    "        if predicted_class == test_y[i]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "\n",
    "#8 epochs accuracy 25.743\n",
    "#9 epochs accuracy 25.106\n",
    "print(\"Accuracy: \", round((correct/total)*100, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 253 253 ... 252 253 228]\n",
      " [255 253 254 ... 253 252 225]\n",
      " [255 254 253 ... 253 252 224]\n",
      " ...\n",
      " [200 176  48 ...  47  53  47]\n",
      " [184 156  36 ...  38  37  35]\n",
      " [178 137  29 ...  25  17  31]]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(training_data[3][0])\n",
    "print(len(training_data[3][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:04<00:00, 31.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  90.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#testing on the training data \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "batch = 10\n",
    "\n",
    "#train_X , train_y = train_X.to(device), train_y.to(device)\n",
    "with torch.no_grad():\n",
    "    #for i in tqdm(range(len(train_X))):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 52, 300)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        outs = network(batch_X)\n",
    "        \n",
    "        for count, j in enumerate(outs):\n",
    "            temp = torch.argmax(j)\n",
    "           \n",
    "            if temp == batch_y[count] :\n",
    "                #print(predicted_class, train_y[i])\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round((correct/total)*100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11])\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1060][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6742, 0.2223, 0.9260],\n",
      "        [0.3503, 0.8297, 0.1198]])\n",
      "tensor([[[0.6742, 0.2223],\n",
      "         [0.9260, 0.3503],\n",
      "         [0.8297, 0.1198]]])\n",
      "tensor(0.6742)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "r = torch.rand(2,3)\n",
    "t = r.view(-1, 3, 2)\n",
    "\n",
    "print (r)\n",
    "print(t)\n",
    "print(t[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.190\n",
      "Loss after mini-batch   200: 3.181\n",
      "Loss after mini-batch   300: 3.176\n",
      "Loss after mini-batch   400: 3.177\n",
      "Loss after mini-batch   500: 3.176\n",
      "Loss after mini-batch   600: 3.177\n",
      "Loss after mini-batch   700: 3.176\n",
      "Loss after mini-batch   800: 3.175\n",
      "Loss after mini-batch   900: 3.178\n",
      "Loss after mini-batch  1000: 3.174\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.169\n",
      "Loss after mini-batch   200: 3.171\n",
      "Loss after mini-batch   300: 3.168\n",
      "Loss after mini-batch   400: 3.166\n",
      "Loss after mini-batch   500: 3.163\n",
      "Loss after mini-batch   600: 3.164\n",
      "Loss after mini-batch   700: 3.159\n",
      "Loss after mini-batch   800: 3.155\n",
      "Loss after mini-batch   900: 3.156\n",
      "Loss after mini-batch  1000: 3.146\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.140\n",
      "Loss after mini-batch   200: 3.131\n",
      "Loss after mini-batch   300: 3.118\n",
      "Loss after mini-batch   400: 3.110\n",
      "Loss after mini-batch   500: 3.091\n",
      "Loss after mini-batch   600: 3.086\n",
      "Loss after mini-batch   700: 3.059\n",
      "Loss after mini-batch   800: 3.045\n",
      "Loss after mini-batch   900: 3.035\n",
      "Loss after mini-batch  1000: 2.993\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.955\n",
      "Loss after mini-batch   200: 2.939\n",
      "Loss after mini-batch   300: 2.904\n",
      "Loss after mini-batch   400: 2.892\n",
      "Loss after mini-batch   500: 2.884\n",
      "Loss after mini-batch   600: 2.856\n",
      "Loss after mini-batch   700: 2.821\n",
      "Loss after mini-batch   800: 2.825\n",
      "Loss after mini-batch   900: 2.812\n",
      "Loss after mini-batch  1000: 2.785\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.683\n",
      "Loss after mini-batch   200: 2.704\n",
      "Loss after mini-batch   300: 2.686\n",
      "Loss after mini-batch   400: 2.680\n",
      "Loss after mini-batch   500: 2.663\n",
      "Loss after mini-batch   600: 2.643\n",
      "Loss after mini-batch   700: 2.613\n",
      "Loss after mini-batch   800: 2.572\n",
      "Loss after mini-batch   900: 2.608\n",
      "Loss after mini-batch  1000: 2.541\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.512\n",
      "Loss after mini-batch   200: 2.470\n",
      "Loss after mini-batch   300: 2.461\n",
      "Loss after mini-batch   400: 2.479\n",
      "Loss after mini-batch   500: 2.466\n",
      "Loss after mini-batch   600: 2.491\n",
      "Loss after mini-batch   700: 2.454\n",
      "Loss after mini-batch   800: 2.425\n",
      "Loss after mini-batch   900: 2.453\n",
      "Loss after mini-batch  1000: 2.454\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.339\n",
      "Loss after mini-batch   200: 2.368\n",
      "Loss after mini-batch   300: 2.346\n",
      "Loss after mini-batch   400: 2.375\n",
      "Loss after mini-batch   500: 2.353\n",
      "Loss after mini-batch   600: 2.344\n",
      "Loss after mini-batch   700: 2.361\n",
      "Loss after mini-batch   800: 2.348\n",
      "Loss after mini-batch   900: 2.374\n",
      "Loss after mini-batch  1000: 2.342\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.282\n",
      "Loss after mini-batch   200: 2.245\n",
      "Loss after mini-batch   300: 2.274\n",
      "Loss after mini-batch   400: 2.254\n",
      "Loss after mini-batch   500: 2.293\n",
      "Loss after mini-batch   600: 2.247\n",
      "Loss after mini-batch   700: 2.296\n",
      "Loss after mini-batch   800: 2.280\n",
      "Loss after mini-batch   900: 2.284\n",
      "Loss after mini-batch  1000: 2.252\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.145\n",
      "Loss after mini-batch   200: 2.155\n",
      "Loss after mini-batch   300: 2.137\n",
      "Loss after mini-batch   400: 2.156\n",
      "Loss after mini-batch   500: 2.209\n",
      "Loss after mini-batch   600: 2.171\n",
      "Loss after mini-batch   700: 2.207\n",
      "Loss after mini-batch   800: 2.228\n",
      "Loss after mini-batch   900: 2.214\n",
      "Loss after mini-batch  1000: 2.233\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.045\n",
      "Loss after mini-batch   200: 2.049\n",
      "Loss after mini-batch   300: 2.100\n",
      "Loss after mini-batch   400: 2.100\n",
      "Loss after mini-batch   500: 2.114\n",
      "Loss after mini-batch   600: 2.044\n",
      "Loss after mini-batch   700: 2.119\n",
      "Loss after mini-batch   800: 2.143\n",
      "Loss after mini-batch   900: 2.142\n",
      "Loss after mini-batch  1000: 2.145\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.948\n",
      "Loss after mini-batch   200: 1.983\n",
      "Loss after mini-batch   300: 2.029\n",
      "Loss after mini-batch   400: 2.061\n",
      "Loss after mini-batch   500: 2.049\n",
      "Loss after mini-batch   600: 1.998\n",
      "Loss after mini-batch   700: 2.011\n",
      "Loss after mini-batch   800: 2.028\n",
      "Loss after mini-batch   900: 2.026\n",
      "Loss after mini-batch  1000: 2.015\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.851\n",
      "Loss after mini-batch   200: 1.886\n",
      "Loss after mini-batch   300: 1.917\n",
      "Loss after mini-batch   400: 1.913\n",
      "Loss after mini-batch   500: 1.935\n",
      "Loss after mini-batch   600: 1.943\n",
      "Loss after mini-batch   700: 1.947\n",
      "Loss after mini-batch   800: 1.942\n",
      "Loss after mini-batch   900: 1.939\n",
      "Loss after mini-batch  1000: 2.009\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.784\n",
      "Loss after mini-batch   200: 1.789\n",
      "Loss after mini-batch   300: 1.850\n",
      "Loss after mini-batch   400: 1.821\n",
      "Loss after mini-batch   500: 1.799\n",
      "Loss after mini-batch   600: 1.843\n",
      "Loss after mini-batch   700: 1.841\n",
      "Loss after mini-batch   800: 1.893\n",
      "Loss after mini-batch   900: 1.847\n",
      "Loss after mini-batch  1000: 1.876\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.661\n",
      "Loss after mini-batch   200: 1.658\n",
      "Loss after mini-batch   300: 1.678\n",
      "Loss after mini-batch   400: 1.766\n",
      "Loss after mini-batch   500: 1.728\n",
      "Loss after mini-batch   600: 1.784\n",
      "Loss after mini-batch   700: 1.776\n",
      "Loss after mini-batch   800: 1.774\n",
      "Loss after mini-batch   900: 1.820\n",
      "Loss after mini-batch  1000: 1.843\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.553\n",
      "Loss after mini-batch   200: 1.564\n",
      "Loss after mini-batch   300: 1.597\n",
      "Loss after mini-batch   400: 1.611\n",
      "Loss after mini-batch   500: 1.604\n",
      "Loss after mini-batch   600: 1.658\n",
      "Loss after mini-batch   700: 1.652\n",
      "Loss after mini-batch   800: 1.684\n",
      "Loss after mini-batch   900: 1.700\n",
      "Loss after mini-batch  1000: 1.703\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.536\n",
      "Loss after mini-batch   200: 1.489\n",
      "Loss after mini-batch   300: 1.511\n",
      "Loss after mini-batch   400: 1.538\n",
      "Loss after mini-batch   500: 1.527\n",
      "Loss after mini-batch   600: 1.512\n",
      "Loss after mini-batch   700: 1.554\n",
      "Loss after mini-batch   800: 1.576\n",
      "Loss after mini-batch   900: 1.604\n",
      "Loss after mini-batch  1000: 1.595\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.308\n",
      "Loss after mini-batch   200: 1.419\n",
      "Loss after mini-batch   300: 1.407\n",
      "Loss after mini-batch   400: 1.410\n",
      "Loss after mini-batch   500: 1.422\n",
      "Loss after mini-batch   600: 1.489\n",
      "Loss after mini-batch   700: 1.464\n",
      "Loss after mini-batch   800: 1.495\n",
      "Loss after mini-batch   900: 1.496\n",
      "Loss after mini-batch  1000: 1.530\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.245\n",
      "Loss after mini-batch   200: 1.241\n",
      "Loss after mini-batch   300: 1.294\n",
      "Loss after mini-batch   400: 1.308\n",
      "Loss after mini-batch   500: 1.331\n",
      "Loss after mini-batch   600: 1.345\n",
      "Loss after mini-batch   700: 1.405\n",
      "Loss after mini-batch   800: 1.373\n",
      "Loss after mini-batch   900: 1.394\n",
      "Loss after mini-batch  1000: 1.441\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.154\n",
      "Loss after mini-batch   200: 1.184\n",
      "Loss after mini-batch   300: 1.197\n",
      "Loss after mini-batch   400: 1.222\n",
      "Loss after mini-batch   500: 1.228\n",
      "Loss after mini-batch   600: 1.203\n",
      "Loss after mini-batch   700: 1.236\n",
      "Loss after mini-batch   800: 1.251\n",
      "Loss after mini-batch   900: 1.306\n",
      "Loss after mini-batch  1000: 1.278\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.026\n",
      "Loss after mini-batch   200: 1.052\n",
      "Loss after mini-batch   300: 1.112\n",
      "Loss after mini-batch   400: 1.102\n",
      "Loss after mini-batch   500: 1.108\n",
      "Loss after mini-batch   600: 1.099\n",
      "Loss after mini-batch   700: 1.121\n",
      "Loss after mini-batch   800: 1.136\n",
      "Loss after mini-batch   900: 1.181\n",
      "Loss after mini-batch  1000: 1.216\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.932\n",
      "Loss after mini-batch   200: 0.964\n",
      "Loss after mini-batch   300: 0.965\n",
      "Loss after mini-batch   400: 0.974\n",
      "Loss after mini-batch   500: 0.992\n",
      "Loss after mini-batch   600: 0.994\n",
      "Loss after mini-batch   700: 1.059\n",
      "Loss after mini-batch   800: 1.080\n",
      "Loss after mini-batch   900: 1.071\n",
      "Loss after mini-batch  1000: 1.102\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.847\n",
      "Loss after mini-batch   200: 0.860\n",
      "Loss after mini-batch   300: 0.849\n",
      "Loss after mini-batch   400: 0.900\n",
      "Loss after mini-batch   500: 0.917\n",
      "Loss after mini-batch   600: 0.901\n",
      "Loss after mini-batch   700: 0.927\n",
      "Loss after mini-batch   800: 0.954\n",
      "Loss after mini-batch   900: 0.970\n",
      "Loss after mini-batch  1000: 0.969\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.752\n",
      "Loss after mini-batch   200: 0.754\n",
      "Loss after mini-batch   300: 0.804\n",
      "Loss after mini-batch   400: 0.802\n",
      "Loss after mini-batch   500: 0.831\n",
      "Loss after mini-batch   600: 0.817\n",
      "Loss after mini-batch   700: 0.871\n",
      "Loss after mini-batch   800: 0.873\n",
      "Loss after mini-batch   900: 0.871\n",
      "Loss after mini-batch  1000: 0.922\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.720\n",
      "Loss after mini-batch   200: 0.713\n",
      "Loss after mini-batch   300: 0.702\n",
      "Loss after mini-batch   400: 0.750\n",
      "Loss after mini-batch   500: 0.706\n",
      "Loss after mini-batch   600: 0.741\n",
      "Loss after mini-batch   700: 0.731\n",
      "Loss after mini-batch   800: 0.782\n",
      "Loss after mini-batch   900: 0.793\n",
      "Loss after mini-batch  1000: 0.796\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.614\n",
      "Loss after mini-batch   200: 0.644\n",
      "Loss after mini-batch   300: 0.615\n",
      "Loss after mini-batch   400: 0.621\n",
      "Loss after mini-batch   500: 0.676\n",
      "Loss after mini-batch   600: 0.700\n",
      "Loss after mini-batch   700: 0.690\n",
      "Loss after mini-batch   800: 0.687\n",
      "Loss after mini-batch   900: 0.705\n",
      "Loss after mini-batch  1000: 0.724\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 0: 22 %\n",
      "--------------------------------\n",
      "791 3554\n",
      "targets 2 2\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.188\n",
      "Loss after mini-batch   200: 3.179\n",
      "Loss after mini-batch   300: 3.181\n",
      "Loss after mini-batch   400: 3.179\n",
      "Loss after mini-batch   500: 3.177\n",
      "Loss after mini-batch   600: 3.178\n",
      "Loss after mini-batch   700: 3.177\n",
      "Loss after mini-batch   800: 3.177\n",
      "Loss after mini-batch   900: 3.179\n",
      "Loss after mini-batch  1000: 3.176\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.176\n",
      "Loss after mini-batch   200: 3.170\n",
      "Loss after mini-batch   300: 3.175\n",
      "Loss after mini-batch   400: 3.176\n",
      "Loss after mini-batch   500: 3.172\n",
      "Loss after mini-batch   600: 3.172\n",
      "Loss after mini-batch   700: 3.172\n",
      "Loss after mini-batch   800: 3.173\n",
      "Loss after mini-batch   900: 3.170\n",
      "Loss after mini-batch  1000: 3.167\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.164\n",
      "Loss after mini-batch   200: 3.161\n",
      "Loss after mini-batch   300: 3.162\n",
      "Loss after mini-batch   400: 3.155\n",
      "Loss after mini-batch   500: 3.151\n",
      "Loss after mini-batch   600: 3.149\n",
      "Loss after mini-batch   700: 3.141\n",
      "Loss after mini-batch   800: 3.139\n",
      "Loss after mini-batch   900: 3.133\n",
      "Loss after mini-batch  1000: 3.122\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 3.102\n",
      "Loss after mini-batch   200: 3.086\n",
      "Loss after mini-batch   300: 3.072\n",
      "Loss after mini-batch   400: 3.049\n",
      "Loss after mini-batch   500: 3.033\n",
      "Loss after mini-batch   600: 3.011\n",
      "Loss after mini-batch   700: 2.987\n",
      "Loss after mini-batch   800: 2.969\n",
      "Loss after mini-batch   900: 2.933\n",
      "Loss after mini-batch  1000: 2.904\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.870\n",
      "Loss after mini-batch   200: 2.880\n",
      "Loss after mini-batch   300: 2.828\n",
      "Loss after mini-batch   400: 2.792\n",
      "Loss after mini-batch   500: 2.817\n",
      "Loss after mini-batch   600: 2.783\n",
      "Loss after mini-batch   700: 2.766\n",
      "Loss after mini-batch   800: 2.753\n",
      "Loss after mini-batch   900: 2.721\n",
      "Loss after mini-batch  1000: 2.720\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.648\n",
      "Loss after mini-batch   200: 2.617\n",
      "Loss after mini-batch   300: 2.628\n",
      "Loss after mini-batch   400: 2.607\n",
      "Loss after mini-batch   500: 2.593\n",
      "Loss after mini-batch   600: 2.616\n",
      "Loss after mini-batch   700: 2.520\n",
      "Loss after mini-batch   800: 2.549\n",
      "Loss after mini-batch   900: 2.535\n",
      "Loss after mini-batch  1000: 2.567\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.470\n",
      "Loss after mini-batch   200: 2.496\n",
      "Loss after mini-batch   300: 2.477\n",
      "Loss after mini-batch   400: 2.462\n",
      "Loss after mini-batch   500: 2.427\n",
      "Loss after mini-batch   600: 2.415\n",
      "Loss after mini-batch   700: 2.423\n",
      "Loss after mini-batch   800: 2.399\n",
      "Loss after mini-batch   900: 2.441\n",
      "Loss after mini-batch  1000: 2.407\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.321\n",
      "Loss after mini-batch   200: 2.330\n",
      "Loss after mini-batch   300: 2.285\n",
      "Loss after mini-batch   400: 2.336\n",
      "Loss after mini-batch   500: 2.323\n",
      "Loss after mini-batch   600: 2.365\n",
      "Loss after mini-batch   700: 2.352\n",
      "Loss after mini-batch   800: 2.338\n",
      "Loss after mini-batch   900: 2.316\n",
      "Loss after mini-batch  1000: 2.335\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.221\n",
      "Loss after mini-batch   200: 2.255\n",
      "Loss after mini-batch   300: 2.250\n",
      "Loss after mini-batch   400: 2.213\n",
      "Loss after mini-batch   500: 2.248\n",
      "Loss after mini-batch   600: 2.229\n",
      "Loss after mini-batch   700: 2.221\n",
      "Loss after mini-batch   800: 2.298\n",
      "Loss after mini-batch   900: 2.274\n",
      "Loss after mini-batch  1000: 2.292\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.143\n",
      "Loss after mini-batch   200: 2.130\n",
      "Loss after mini-batch   300: 2.106\n",
      "Loss after mini-batch   400: 2.170\n",
      "Loss after mini-batch   500: 2.195\n",
      "Loss after mini-batch   600: 2.159\n",
      "Loss after mini-batch   700: 2.189\n",
      "Loss after mini-batch   800: 2.179\n",
      "Loss after mini-batch   900: 2.191\n",
      "Loss after mini-batch  1000: 2.182\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 2.050\n",
      "Loss after mini-batch   200: 2.018\n",
      "Loss after mini-batch   300: 2.049\n",
      "Loss after mini-batch   400: 2.077\n",
      "Loss after mini-batch   500: 2.091\n",
      "Loss after mini-batch   600: 2.103\n",
      "Loss after mini-batch   700: 2.081\n",
      "Loss after mini-batch   800: 2.125\n",
      "Loss after mini-batch   900: 2.078\n",
      "Loss after mini-batch  1000: 2.107\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.955\n",
      "Loss after mini-batch   200: 1.930\n",
      "Loss after mini-batch   300: 1.961\n",
      "Loss after mini-batch   400: 1.968\n",
      "Loss after mini-batch   500: 1.991\n",
      "Loss after mini-batch   600: 1.999\n",
      "Loss after mini-batch   700: 2.020\n",
      "Loss after mini-batch   800: 2.055\n",
      "Loss after mini-batch   900: 2.037\n",
      "Loss after mini-batch  1000: 2.008\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.841\n",
      "Loss after mini-batch   200: 1.855\n",
      "Loss after mini-batch   300: 1.857\n",
      "Loss after mini-batch   400: 1.927\n",
      "Loss after mini-batch   500: 1.891\n",
      "Loss after mini-batch   600: 1.960\n",
      "Loss after mini-batch   700: 1.946\n",
      "Loss after mini-batch   800: 1.911\n",
      "Loss after mini-batch   900: 1.984\n",
      "Loss after mini-batch  1000: 1.961\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.727\n",
      "Loss after mini-batch   200: 1.754\n",
      "Loss after mini-batch   300: 1.784\n",
      "Loss after mini-batch   400: 1.779\n",
      "Loss after mini-batch   500: 1.827\n",
      "Loss after mini-batch   600: 1.813\n",
      "Loss after mini-batch   700: 1.845\n",
      "Loss after mini-batch   800: 1.838\n",
      "Loss after mini-batch   900: 1.830\n",
      "Loss after mini-batch  1000: 1.870\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.668\n",
      "Loss after mini-batch   200: 1.690\n",
      "Loss after mini-batch   300: 1.607\n",
      "Loss after mini-batch   400: 1.685\n",
      "Loss after mini-batch   500: 1.716\n",
      "Loss after mini-batch   600: 1.719\n",
      "Loss after mini-batch   700: 1.756\n",
      "Loss after mini-batch   800: 1.749\n",
      "Loss after mini-batch   900: 1.808\n",
      "Loss after mini-batch  1000: 1.801\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.490\n",
      "Loss after mini-batch   200: 1.533\n",
      "Loss after mini-batch   300: 1.590\n",
      "Loss after mini-batch   400: 1.626\n",
      "Loss after mini-batch   500: 1.618\n",
      "Loss after mini-batch   600: 1.613\n",
      "Loss after mini-batch   700: 1.619\n",
      "Loss after mini-batch   800: 1.676\n",
      "Loss after mini-batch   900: 1.708\n",
      "Loss after mini-batch  1000: 1.706\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.452\n",
      "Loss after mini-batch   200: 1.454\n",
      "Loss after mini-batch   300: 1.462\n",
      "Loss after mini-batch   400: 1.518\n",
      "Loss after mini-batch   500: 1.508\n",
      "Loss after mini-batch   600: 1.492\n",
      "Loss after mini-batch   700: 1.564\n",
      "Loss after mini-batch   800: 1.597\n",
      "Loss after mini-batch   900: 1.579\n",
      "Loss after mini-batch  1000: 1.608\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.340\n",
      "Loss after mini-batch   200: 1.324\n",
      "Loss after mini-batch   300: 1.346\n",
      "Loss after mini-batch   400: 1.431\n",
      "Loss after mini-batch   500: 1.401\n",
      "Loss after mini-batch   600: 1.419\n",
      "Loss after mini-batch   700: 1.441\n",
      "Loss after mini-batch   800: 1.438\n",
      "Loss after mini-batch   900: 1.450\n",
      "Loss after mini-batch  1000: 1.500\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.255\n",
      "Loss after mini-batch   200: 1.248\n",
      "Loss after mini-batch   300: 1.233\n",
      "Loss after mini-batch   400: 1.260\n",
      "Loss after mini-batch   500: 1.304\n",
      "Loss after mini-batch   600: 1.311\n",
      "Loss after mini-batch   700: 1.346\n",
      "Loss after mini-batch   800: 1.349\n",
      "Loss after mini-batch   900: 1.422\n",
      "Loss after mini-batch  1000: 1.377\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.140\n",
      "Loss after mini-batch   200: 1.108\n",
      "Loss after mini-batch   300: 1.156\n",
      "Loss after mini-batch   400: 1.213\n",
      "Loss after mini-batch   500: 1.206\n",
      "Loss after mini-batch   600: 1.201\n",
      "Loss after mini-batch   700: 1.220\n",
      "Loss after mini-batch   800: 1.269\n",
      "Loss after mini-batch   900: 1.257\n",
      "Loss after mini-batch  1000: 1.324\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 1.029\n",
      "Loss after mini-batch   200: 1.000\n",
      "Loss after mini-batch   300: 1.075\n",
      "Loss after mini-batch   400: 1.058\n",
      "Loss after mini-batch   500: 1.104\n",
      "Loss after mini-batch   600: 1.161\n",
      "Loss after mini-batch   700: 1.154\n",
      "Loss after mini-batch   800: 1.143\n",
      "Loss after mini-batch   900: 1.180\n",
      "Loss after mini-batch  1000: 1.203\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.962\n",
      "Loss after mini-batch   200: 0.961\n",
      "Loss after mini-batch   300: 0.951\n",
      "Loss after mini-batch   400: 0.988\n",
      "Loss after mini-batch   500: 1.036\n",
      "Loss after mini-batch   600: 1.018\n",
      "Loss after mini-batch   700: 1.062\n",
      "Loss after mini-batch   800: 1.057\n",
      "Loss after mini-batch   900: 1.059\n",
      "Loss after mini-batch  1000: 1.114\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.858\n",
      "Loss after mini-batch   200: 0.842\n",
      "Loss after mini-batch   300: 0.870\n",
      "Loss after mini-batch   400: 0.899\n",
      "Loss after mini-batch   500: 0.942\n",
      "Loss after mini-batch   600: 0.938\n",
      "Loss after mini-batch   700: 0.972\n",
      "Loss after mini-batch   800: 0.985\n",
      "Loss after mini-batch   900: 1.008\n",
      "Loss after mini-batch  1000: 1.023\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.775\n",
      "Loss after mini-batch   200: 0.818\n",
      "Loss after mini-batch   300: 0.811\n",
      "Loss after mini-batch   400: 0.850\n",
      "Loss after mini-batch   500: 0.856\n",
      "Loss after mini-batch   600: 0.846\n",
      "Loss after mini-batch   700: 0.859\n",
      "Loss after mini-batch   800: 0.894\n",
      "Loss after mini-batch   900: 0.885\n",
      "Loss after mini-batch  1000: 0.879\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.760\n",
      "Loss after mini-batch   200: 0.706\n",
      "Loss after mini-batch   300: 0.721\n",
      "Loss after mini-batch   400: 0.727\n",
      "Loss after mini-batch   500: 0.761\n",
      "Loss after mini-batch   600: 0.780\n",
      "Loss after mini-batch   700: 0.780\n",
      "Loss after mini-batch   800: 0.818\n",
      "Loss after mini-batch   900: 0.799\n",
      "Loss after mini-batch  1000: 0.855\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 1: 19 %\n",
      "--------------------------------\n",
      "686 3553\n",
      "targets 1 1\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.210\n",
      "Loss after mini-batch   200: 3.179\n",
      "Loss after mini-batch   300: 3.178\n",
      "Loss after mini-batch   400: 3.181\n",
      "Loss after mini-batch   500: 3.178\n",
      "Loss after mini-batch   600: 3.181\n",
      "Loss after mini-batch   700: 3.179\n",
      "Loss after mini-batch   800: 3.177\n",
      "Loss after mini-batch   900: 3.176\n",
      "Loss after mini-batch  1000: 3.175\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.172\n",
      "Loss after mini-batch   200: 3.171\n",
      "Loss after mini-batch   300: 3.172\n",
      "Loss after mini-batch   400: 3.168\n",
      "Loss after mini-batch   500: 3.169\n",
      "Loss after mini-batch   600: 3.167\n",
      "Loss after mini-batch   700: 3.163\n",
      "Loss after mini-batch   800: 3.163\n",
      "Loss after mini-batch   900: 3.157\n",
      "Loss after mini-batch  1000: 3.152\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.149\n",
      "Loss after mini-batch   200: 3.138\n",
      "Loss after mini-batch   300: 3.130\n",
      "Loss after mini-batch   400: 3.119\n",
      "Loss after mini-batch   500: 3.111\n",
      "Loss after mini-batch   600: 3.103\n",
      "Loss after mini-batch   700: 3.081\n",
      "Loss after mini-batch   800: 3.063\n",
      "Loss after mini-batch   900: 3.038\n",
      "Loss after mini-batch  1000: 3.022\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 3.003\n",
      "Loss after mini-batch   200: 2.971\n",
      "Loss after mini-batch   300: 2.956\n",
      "Loss after mini-batch   400: 2.930\n",
      "Loss after mini-batch   500: 2.922\n",
      "Loss after mini-batch   600: 2.913\n",
      "Loss after mini-batch   700: 2.908\n",
      "Loss after mini-batch   800: 2.851\n",
      "Loss after mini-batch   900: 2.877\n",
      "Loss after mini-batch  1000: 2.887\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.834\n",
      "Loss after mini-batch   200: 2.766\n",
      "Loss after mini-batch   300: 2.795\n",
      "Loss after mini-batch   400: 2.787\n",
      "Loss after mini-batch   500: 2.779\n",
      "Loss after mini-batch   600: 2.743\n",
      "Loss after mini-batch   700: 2.745\n",
      "Loss after mini-batch   800: 2.736\n",
      "Loss after mini-batch   900: 2.722\n",
      "Loss after mini-batch  1000: 2.702\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.645\n",
      "Loss after mini-batch   200: 2.617\n",
      "Loss after mini-batch   300: 2.594\n",
      "Loss after mini-batch   400: 2.588\n",
      "Loss after mini-batch   500: 2.539\n",
      "Loss after mini-batch   600: 2.549\n",
      "Loss after mini-batch   700: 2.556\n",
      "Loss after mini-batch   800: 2.537\n",
      "Loss after mini-batch   900: 2.552\n",
      "Loss after mini-batch  1000: 2.533\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.453\n",
      "Loss after mini-batch   200: 2.421\n",
      "Loss after mini-batch   300: 2.413\n",
      "Loss after mini-batch   400: 2.422\n",
      "Loss after mini-batch   500: 2.400\n",
      "Loss after mini-batch   600: 2.424\n",
      "Loss after mini-batch   700: 2.407\n",
      "Loss after mini-batch   800: 2.396\n",
      "Loss after mini-batch   900: 2.402\n",
      "Loss after mini-batch  1000: 2.397\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.264\n",
      "Loss after mini-batch   200: 2.330\n",
      "Loss after mini-batch   300: 2.286\n",
      "Loss after mini-batch   400: 2.295\n",
      "Loss after mini-batch   500: 2.328\n",
      "Loss after mini-batch   600: 2.328\n",
      "Loss after mini-batch   700: 2.357\n",
      "Loss after mini-batch   800: 2.343\n",
      "Loss after mini-batch   900: 2.329\n",
      "Loss after mini-batch  1000: 2.328\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.222\n",
      "Loss after mini-batch   200: 2.206\n",
      "Loss after mini-batch   300: 2.223\n",
      "Loss after mini-batch   400: 2.186\n",
      "Loss after mini-batch   500: 2.243\n",
      "Loss after mini-batch   600: 2.263\n",
      "Loss after mini-batch   700: 2.211\n",
      "Loss after mini-batch   800: 2.241\n",
      "Loss after mini-batch   900: 2.223\n",
      "Loss after mini-batch  1000: 2.270\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.094\n",
      "Loss after mini-batch   200: 2.090\n",
      "Loss after mini-batch   300: 2.135\n",
      "Loss after mini-batch   400: 2.123\n",
      "Loss after mini-batch   500: 2.092\n",
      "Loss after mini-batch   600: 2.139\n",
      "Loss after mini-batch   700: 2.175\n",
      "Loss after mini-batch   800: 2.176\n",
      "Loss after mini-batch   900: 2.146\n",
      "Loss after mini-batch  1000: 2.172\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 2.002\n",
      "Loss after mini-batch   200: 1.994\n",
      "Loss after mini-batch   300: 1.984\n",
      "Loss after mini-batch   400: 2.038\n",
      "Loss after mini-batch   500: 2.049\n",
      "Loss after mini-batch   600: 2.039\n",
      "Loss after mini-batch   700: 2.084\n",
      "Loss after mini-batch   800: 2.069\n",
      "Loss after mini-batch   900: 2.113\n",
      "Loss after mini-batch  1000: 2.093\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.883\n",
      "Loss after mini-batch   200: 1.923\n",
      "Loss after mini-batch   300: 1.897\n",
      "Loss after mini-batch   400: 1.934\n",
      "Loss after mini-batch   500: 1.948\n",
      "Loss after mini-batch   600: 1.958\n",
      "Loss after mini-batch   700: 1.972\n",
      "Loss after mini-batch   800: 1.972\n",
      "Loss after mini-batch   900: 2.032\n",
      "Loss after mini-batch  1000: 2.018\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.815\n",
      "Loss after mini-batch   200: 1.822\n",
      "Loss after mini-batch   300: 1.817\n",
      "Loss after mini-batch   400: 1.809\n",
      "Loss after mini-batch   500: 1.833\n",
      "Loss after mini-batch   600: 1.887\n",
      "Loss after mini-batch   700: 1.893\n",
      "Loss after mini-batch   800: 1.878\n",
      "Loss after mini-batch   900: 1.898\n",
      "Loss after mini-batch  1000: 1.901\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.670\n",
      "Loss after mini-batch   200: 1.716\n",
      "Loss after mini-batch   300: 1.721\n",
      "Loss after mini-batch   400: 1.741\n",
      "Loss after mini-batch   500: 1.735\n",
      "Loss after mini-batch   600: 1.808\n",
      "Loss after mini-batch   700: 1.789\n",
      "Loss after mini-batch   800: 1.801\n",
      "Loss after mini-batch   900: 1.808\n",
      "Loss after mini-batch  1000: 1.820\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.567\n",
      "Loss after mini-batch   200: 1.524\n",
      "Loss after mini-batch   300: 1.596\n",
      "Loss after mini-batch   400: 1.611\n",
      "Loss after mini-batch   500: 1.627\n",
      "Loss after mini-batch   600: 1.660\n",
      "Loss after mini-batch   700: 1.720\n",
      "Loss after mini-batch   800: 1.697\n",
      "Loss after mini-batch   900: 1.713\n",
      "Loss after mini-batch  1000: 1.788\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.439\n",
      "Loss after mini-batch   200: 1.481\n",
      "Loss after mini-batch   300: 1.508\n",
      "Loss after mini-batch   400: 1.575\n",
      "Loss after mini-batch   500: 1.505\n",
      "Loss after mini-batch   600: 1.506\n",
      "Loss after mini-batch   700: 1.588\n",
      "Loss after mini-batch   800: 1.635\n",
      "Loss after mini-batch   900: 1.609\n",
      "Loss after mini-batch  1000: 1.664\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.348\n",
      "Loss after mini-batch   200: 1.369\n",
      "Loss after mini-batch   300: 1.396\n",
      "Loss after mini-batch   400: 1.392\n",
      "Loss after mini-batch   500: 1.419\n",
      "Loss after mini-batch   600: 1.406\n",
      "Loss after mini-batch   700: 1.494\n",
      "Loss after mini-batch   800: 1.477\n",
      "Loss after mini-batch   900: 1.535\n",
      "Loss after mini-batch  1000: 1.523\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.258\n",
      "Loss after mini-batch   200: 1.252\n",
      "Loss after mini-batch   300: 1.308\n",
      "Loss after mini-batch   400: 1.309\n",
      "Loss after mini-batch   500: 1.304\n",
      "Loss after mini-batch   600: 1.314\n",
      "Loss after mini-batch   700: 1.329\n",
      "Loss after mini-batch   800: 1.355\n",
      "Loss after mini-batch   900: 1.404\n",
      "Loss after mini-batch  1000: 1.399\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.165\n",
      "Loss after mini-batch   200: 1.118\n",
      "Loss after mini-batch   300: 1.169\n",
      "Loss after mini-batch   400: 1.184\n",
      "Loss after mini-batch   500: 1.202\n",
      "Loss after mini-batch   600: 1.271\n",
      "Loss after mini-batch   700: 1.258\n",
      "Loss after mini-batch   800: 1.279\n",
      "Loss after mini-batch   900: 1.287\n",
      "Loss after mini-batch  1000: 1.347\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.071\n",
      "Loss after mini-batch   200: 1.099\n",
      "Loss after mini-batch   300: 1.072\n",
      "Loss after mini-batch   400: 1.102\n",
      "Loss after mini-batch   500: 1.082\n",
      "Loss after mini-batch   600: 1.146\n",
      "Loss after mini-batch   700: 1.152\n",
      "Loss after mini-batch   800: 1.163\n",
      "Loss after mini-batch   900: 1.182\n",
      "Loss after mini-batch  1000: 1.210\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.972\n",
      "Loss after mini-batch   200: 0.946\n",
      "Loss after mini-batch   300: 0.995\n",
      "Loss after mini-batch   400: 1.028\n",
      "Loss after mini-batch   500: 1.041\n",
      "Loss after mini-batch   600: 1.092\n",
      "Loss after mini-batch   700: 1.058\n",
      "Loss after mini-batch   800: 1.085\n",
      "Loss after mini-batch   900: 1.104\n",
      "Loss after mini-batch  1000: 1.061\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.855\n",
      "Loss after mini-batch   200: 0.871\n",
      "Loss after mini-batch   300: 0.891\n",
      "Loss after mini-batch   400: 0.931\n",
      "Loss after mini-batch   500: 0.929\n",
      "Loss after mini-batch   600: 0.999\n",
      "Loss after mini-batch   700: 0.961\n",
      "Loss after mini-batch   800: 0.989\n",
      "Loss after mini-batch   900: 0.984\n",
      "Loss after mini-batch  1000: 1.005\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.762\n",
      "Loss after mini-batch   200: 0.772\n",
      "Loss after mini-batch   300: 0.828\n",
      "Loss after mini-batch   400: 0.843\n",
      "Loss after mini-batch   500: 0.813\n",
      "Loss after mini-batch   600: 0.884\n",
      "Loss after mini-batch   700: 0.836\n",
      "Loss after mini-batch   800: 0.874\n",
      "Loss after mini-batch   900: 0.912\n",
      "Loss after mini-batch  1000: 0.948\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.706\n",
      "Loss after mini-batch   200: 0.729\n",
      "Loss after mini-batch   300: 0.718\n",
      "Loss after mini-batch   400: 0.724\n",
      "Loss after mini-batch   500: 0.743\n",
      "Loss after mini-batch   600: 0.774\n",
      "Loss after mini-batch   700: 0.804\n",
      "Loss after mini-batch   800: 0.806\n",
      "Loss after mini-batch   900: 0.808\n",
      "Loss after mini-batch  1000: 0.829\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.646\n",
      "Loss after mini-batch   200: 0.675\n",
      "Loss after mini-batch   300: 0.642\n",
      "Loss after mini-batch   400: 0.680\n",
      "Loss after mini-batch   500: 0.713\n",
      "Loss after mini-batch   600: 0.692\n",
      "Loss after mini-batch   700: 0.684\n",
      "Loss after mini-batch   800: 0.716\n",
      "Loss after mini-batch   900: 0.748\n",
      "Loss after mini-batch  1000: 0.759\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 2: 20 %\n",
      "--------------------------------\n",
      "724 3553\n",
      "targets 1 1\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.185\n",
      "Loss after mini-batch   200: 3.179\n",
      "Loss after mini-batch   300: 3.179\n",
      "Loss after mini-batch   400: 3.178\n",
      "Loss after mini-batch   500: 3.177\n",
      "Loss after mini-batch   600: 3.173\n",
      "Loss after mini-batch   700: 3.173\n",
      "Loss after mini-batch   800: 3.175\n",
      "Loss after mini-batch   900: 3.174\n",
      "Loss after mini-batch  1000: 3.173\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.166\n",
      "Loss after mini-batch   200: 3.167\n",
      "Loss after mini-batch   300: 3.161\n",
      "Loss after mini-batch   400: 3.155\n",
      "Loss after mini-batch   500: 3.152\n",
      "Loss after mini-batch   600: 3.149\n",
      "Loss after mini-batch   700: 3.141\n",
      "Loss after mini-batch   800: 3.126\n",
      "Loss after mini-batch   900: 3.117\n",
      "Loss after mini-batch  1000: 3.100\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.076\n",
      "Loss after mini-batch   200: 3.058\n",
      "Loss after mini-batch   300: 3.018\n",
      "Loss after mini-batch   400: 2.993\n",
      "Loss after mini-batch   500: 2.958\n",
      "Loss after mini-batch   600: 2.916\n",
      "Loss after mini-batch   700: 2.908\n",
      "Loss after mini-batch   800: 2.886\n",
      "Loss after mini-batch   900: 2.833\n",
      "Loss after mini-batch  1000: 2.823\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.721\n",
      "Loss after mini-batch   200: 2.727\n",
      "Loss after mini-batch   300: 2.702\n",
      "Loss after mini-batch   400: 2.673\n",
      "Loss after mini-batch   500: 2.682\n",
      "Loss after mini-batch   600: 2.640\n",
      "Loss after mini-batch   700: 2.646\n",
      "Loss after mini-batch   800: 2.631\n",
      "Loss after mini-batch   900: 2.585\n",
      "Loss after mini-batch  1000: 2.574\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.495\n",
      "Loss after mini-batch   200: 2.489\n",
      "Loss after mini-batch   300: 2.426\n",
      "Loss after mini-batch   400: 2.500\n",
      "Loss after mini-batch   500: 2.475\n",
      "Loss after mini-batch   600: 2.476\n",
      "Loss after mini-batch   700: 2.480\n",
      "Loss after mini-batch   800: 2.478\n",
      "Loss after mini-batch   900: 2.451\n",
      "Loss after mini-batch  1000: 2.476\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.374\n",
      "Loss after mini-batch   200: 2.315\n",
      "Loss after mini-batch   300: 2.342\n",
      "Loss after mini-batch   400: 2.345\n",
      "Loss after mini-batch   500: 2.408\n",
      "Loss after mini-batch   600: 2.376\n",
      "Loss after mini-batch   700: 2.346\n",
      "Loss after mini-batch   800: 2.392\n",
      "Loss after mini-batch   900: 2.390\n",
      "Loss after mini-batch  1000: 2.382\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.256\n",
      "Loss after mini-batch   200: 2.267\n",
      "Loss after mini-batch   300: 2.268\n",
      "Loss after mini-batch   400: 2.272\n",
      "Loss after mini-batch   500: 2.263\n",
      "Loss after mini-batch   600: 2.311\n",
      "Loss after mini-batch   700: 2.285\n",
      "Loss after mini-batch   800: 2.289\n",
      "Loss after mini-batch   900: 2.266\n",
      "Loss after mini-batch  1000: 2.308\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.173\n",
      "Loss after mini-batch   200: 2.186\n",
      "Loss after mini-batch   300: 2.174\n",
      "Loss after mini-batch   400: 2.205\n",
      "Loss after mini-batch   500: 2.190\n",
      "Loss after mini-batch   600: 2.171\n",
      "Loss after mini-batch   700: 2.180\n",
      "Loss after mini-batch   800: 2.215\n",
      "Loss after mini-batch   900: 2.186\n",
      "Loss after mini-batch  1000: 2.197\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.076\n",
      "Loss after mini-batch   200: 2.075\n",
      "Loss after mini-batch   300: 2.055\n",
      "Loss after mini-batch   400: 2.058\n",
      "Loss after mini-batch   500: 2.124\n",
      "Loss after mini-batch   600: 2.124\n",
      "Loss after mini-batch   700: 2.122\n",
      "Loss after mini-batch   800: 2.122\n",
      "Loss after mini-batch   900: 2.094\n",
      "Loss after mini-batch  1000: 2.147\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 1.982\n",
      "Loss after mini-batch   200: 1.972\n",
      "Loss after mini-batch   300: 1.989\n",
      "Loss after mini-batch   400: 2.007\n",
      "Loss after mini-batch   500: 1.975\n",
      "Loss after mini-batch   600: 2.013\n",
      "Loss after mini-batch   700: 1.968\n",
      "Loss after mini-batch   800: 2.030\n",
      "Loss after mini-batch   900: 2.044\n",
      "Loss after mini-batch  1000: 2.049\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.818\n",
      "Loss after mini-batch   200: 1.857\n",
      "Loss after mini-batch   300: 1.880\n",
      "Loss after mini-batch   400: 1.877\n",
      "Loss after mini-batch   500: 1.936\n",
      "Loss after mini-batch   600: 1.936\n",
      "Loss after mini-batch   700: 1.947\n",
      "Loss after mini-batch   800: 1.973\n",
      "Loss after mini-batch   900: 1.949\n",
      "Loss after mini-batch  1000: 1.944\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.759\n",
      "Loss after mini-batch   200: 1.721\n",
      "Loss after mini-batch   300: 1.784\n",
      "Loss after mini-batch   400: 1.778\n",
      "Loss after mini-batch   500: 1.774\n",
      "Loss after mini-batch   600: 1.846\n",
      "Loss after mini-batch   700: 1.851\n",
      "Loss after mini-batch   800: 1.857\n",
      "Loss after mini-batch   900: 1.872\n",
      "Loss after mini-batch  1000: 1.859\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.656\n",
      "Loss after mini-batch   200: 1.677\n",
      "Loss after mini-batch   300: 1.666\n",
      "Loss after mini-batch   400: 1.689\n",
      "Loss after mini-batch   500: 1.681\n",
      "Loss after mini-batch   600: 1.731\n",
      "Loss after mini-batch   700: 1.772\n",
      "Loss after mini-batch   800: 1.757\n",
      "Loss after mini-batch   900: 1.763\n",
      "Loss after mini-batch  1000: 1.719\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.538\n",
      "Loss after mini-batch   200: 1.526\n",
      "Loss after mini-batch   300: 1.587\n",
      "Loss after mini-batch   400: 1.564\n",
      "Loss after mini-batch   500: 1.614\n",
      "Loss after mini-batch   600: 1.601\n",
      "Loss after mini-batch   700: 1.684\n",
      "Loss after mini-batch   800: 1.657\n",
      "Loss after mini-batch   900: 1.699\n",
      "Loss after mini-batch  1000: 1.692\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.444\n",
      "Loss after mini-batch   200: 1.471\n",
      "Loss after mini-batch   300: 1.461\n",
      "Loss after mini-batch   400: 1.468\n",
      "Loss after mini-batch   500: 1.536\n",
      "Loss after mini-batch   600: 1.526\n",
      "Loss after mini-batch   700: 1.531\n",
      "Loss after mini-batch   800: 1.588\n",
      "Loss after mini-batch   900: 1.571\n",
      "Loss after mini-batch  1000: 1.625\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.323\n",
      "Loss after mini-batch   200: 1.390\n",
      "Loss after mini-batch   300: 1.356\n",
      "Loss after mini-batch   400: 1.380\n",
      "Loss after mini-batch   500: 1.404\n",
      "Loss after mini-batch   600: 1.405\n",
      "Loss after mini-batch   700: 1.423\n",
      "Loss after mini-batch   800: 1.493\n",
      "Loss after mini-batch   900: 1.463\n",
      "Loss after mini-batch  1000: 1.522\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.234\n",
      "Loss after mini-batch   200: 1.278\n",
      "Loss after mini-batch   300: 1.278\n",
      "Loss after mini-batch   400: 1.281\n",
      "Loss after mini-batch   500: 1.338\n",
      "Loss after mini-batch   600: 1.362\n",
      "Loss after mini-batch   700: 1.360\n",
      "Loss after mini-batch   800: 1.361\n",
      "Loss after mini-batch   900: 1.372\n",
      "Loss after mini-batch  1000: 1.413\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.159\n",
      "Loss after mini-batch   200: 1.110\n",
      "Loss after mini-batch   300: 1.197\n",
      "Loss after mini-batch   400: 1.213\n",
      "Loss after mini-batch   500: 1.233\n",
      "Loss after mini-batch   600: 1.239\n",
      "Loss after mini-batch   700: 1.236\n",
      "Loss after mini-batch   800: 1.294\n",
      "Loss after mini-batch   900: 1.277\n",
      "Loss after mini-batch  1000: 1.328\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.085\n",
      "Loss after mini-batch   200: 1.055\n",
      "Loss after mini-batch   300: 1.121\n",
      "Loss after mini-batch   400: 1.090\n",
      "Loss after mini-batch   500: 1.105\n",
      "Loss after mini-batch   600: 1.137\n",
      "Loss after mini-batch   700: 1.186\n",
      "Loss after mini-batch   800: 1.187\n",
      "Loss after mini-batch   900: 1.197\n",
      "Loss after mini-batch  1000: 1.238\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 0.973\n",
      "Loss after mini-batch   200: 0.996\n",
      "Loss after mini-batch   300: 1.017\n",
      "Loss after mini-batch   400: 1.005\n",
      "Loss after mini-batch   500: 1.034\n",
      "Loss after mini-batch   600: 1.039\n",
      "Loss after mini-batch   700: 1.103\n",
      "Loss after mini-batch   800: 1.076\n",
      "Loss after mini-batch   900: 1.122\n",
      "Loss after mini-batch  1000: 1.083\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.891\n",
      "Loss after mini-batch   200: 0.914\n",
      "Loss after mini-batch   300: 0.970\n",
      "Loss after mini-batch   400: 0.917\n",
      "Loss after mini-batch   500: 0.977\n",
      "Loss after mini-batch   600: 0.933\n",
      "Loss after mini-batch   700: 0.987\n",
      "Loss after mini-batch   800: 0.995\n",
      "Loss after mini-batch   900: 1.042\n",
      "Loss after mini-batch  1000: 1.003\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.821\n",
      "Loss after mini-batch   200: 0.828\n",
      "Loss after mini-batch   300: 0.873\n",
      "Loss after mini-batch   400: 0.891\n",
      "Loss after mini-batch   500: 0.876\n",
      "Loss after mini-batch   600: 0.930\n",
      "Loss after mini-batch   700: 0.896\n",
      "Loss after mini-batch   800: 0.910\n",
      "Loss after mini-batch   900: 0.946\n",
      "Loss after mini-batch  1000: 0.949\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.731\n",
      "Loss after mini-batch   200: 0.744\n",
      "Loss after mini-batch   300: 0.803\n",
      "Loss after mini-batch   400: 0.789\n",
      "Loss after mini-batch   500: 0.783\n",
      "Loss after mini-batch   600: 0.806\n",
      "Loss after mini-batch   700: 0.860\n",
      "Loss after mini-batch   800: 0.890\n",
      "Loss after mini-batch   900: 0.881\n",
      "Loss after mini-batch  1000: 0.891\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.693\n",
      "Loss after mini-batch   200: 0.698\n",
      "Loss after mini-batch   300: 0.740\n",
      "Loss after mini-batch   400: 0.749\n",
      "Loss after mini-batch   500: 0.751\n",
      "Loss after mini-batch   600: 0.768\n",
      "Loss after mini-batch   700: 0.765\n",
      "Loss after mini-batch   800: 0.791\n",
      "Loss after mini-batch   900: 0.773\n",
      "Loss after mini-batch  1000: 0.809\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.633\n",
      "Loss after mini-batch   200: 0.652\n",
      "Loss after mini-batch   300: 0.657\n",
      "Loss after mini-batch   400: 0.644\n",
      "Loss after mini-batch   500: 0.699\n",
      "Loss after mini-batch   600: 0.715\n",
      "Loss after mini-batch   700: 0.706\n",
      "Loss after mini-batch   800: 0.729\n",
      "Loss after mini-batch   900: 0.764\n",
      "Loss after mini-batch  1000: 0.710\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 3: 21 %\n",
      "--------------------------------\n",
      "770 3553\n",
      "targets 1 1\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.192\n",
      "Loss after mini-batch   200: 3.180\n",
      "Loss after mini-batch   300: 3.179\n",
      "Loss after mini-batch   400: 3.179\n",
      "Loss after mini-batch   500: 3.177\n",
      "Loss after mini-batch   600: 3.177\n",
      "Loss after mini-batch   700: 3.174\n",
      "Loss after mini-batch   800: 3.173\n",
      "Loss after mini-batch   900: 3.171\n",
      "Loss after mini-batch  1000: 3.167\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.162\n",
      "Loss after mini-batch   200: 3.162\n",
      "Loss after mini-batch   300: 3.155\n",
      "Loss after mini-batch   400: 3.151\n",
      "Loss after mini-batch   500: 3.144\n",
      "Loss after mini-batch   600: 3.138\n",
      "Loss after mini-batch   700: 3.126\n",
      "Loss after mini-batch   800: 3.118\n",
      "Loss after mini-batch   900: 3.103\n",
      "Loss after mini-batch  1000: 3.085\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.055\n",
      "Loss after mini-batch   200: 3.028\n",
      "Loss after mini-batch   300: 3.006\n",
      "Loss after mini-batch   400: 2.986\n",
      "Loss after mini-batch   500: 2.976\n",
      "Loss after mini-batch   600: 2.935\n",
      "Loss after mini-batch   700: 2.929\n",
      "Loss after mini-batch   800: 2.907\n",
      "Loss after mini-batch   900: 2.902\n",
      "Loss after mini-batch  1000: 2.887\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.831\n",
      "Loss after mini-batch   200: 2.822\n",
      "Loss after mini-batch   300: 2.805\n",
      "Loss after mini-batch   400: 2.797\n",
      "Loss after mini-batch   500: 2.777\n",
      "Loss after mini-batch   600: 2.770\n",
      "Loss after mini-batch   700: 2.732\n",
      "Loss after mini-batch   800: 2.742\n",
      "Loss after mini-batch   900: 2.711\n",
      "Loss after mini-batch  1000: 2.696\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.607\n",
      "Loss after mini-batch   200: 2.579\n",
      "Loss after mini-batch   300: 2.597\n",
      "Loss after mini-batch   400: 2.575\n",
      "Loss after mini-batch   500: 2.558\n",
      "Loss after mini-batch   600: 2.576\n",
      "Loss after mini-batch   700: 2.553\n",
      "Loss after mini-batch   800: 2.563\n",
      "Loss after mini-batch   900: 2.497\n",
      "Loss after mini-batch  1000: 2.568\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.448\n",
      "Loss after mini-batch   200: 2.440\n",
      "Loss after mini-batch   300: 2.445\n",
      "Loss after mini-batch   400: 2.420\n",
      "Loss after mini-batch   500: 2.417\n",
      "Loss after mini-batch   600: 2.423\n",
      "Loss after mini-batch   700: 2.368\n",
      "Loss after mini-batch   800: 2.427\n",
      "Loss after mini-batch   900: 2.409\n",
      "Loss after mini-batch  1000: 2.416\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.284\n",
      "Loss after mini-batch   200: 2.309\n",
      "Loss after mini-batch   300: 2.299\n",
      "Loss after mini-batch   400: 2.321\n",
      "Loss after mini-batch   500: 2.340\n",
      "Loss after mini-batch   600: 2.327\n",
      "Loss after mini-batch   700: 2.324\n",
      "Loss after mini-batch   800: 2.350\n",
      "Loss after mini-batch   900: 2.331\n",
      "Loss after mini-batch  1000: 2.310\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.194\n",
      "Loss after mini-batch   200: 2.183\n",
      "Loss after mini-batch   300: 2.213\n",
      "Loss after mini-batch   400: 2.214\n",
      "Loss after mini-batch   500: 2.247\n",
      "Loss after mini-batch   600: 2.238\n",
      "Loss after mini-batch   700: 2.252\n",
      "Loss after mini-batch   800: 2.224\n",
      "Loss after mini-batch   900: 2.247\n",
      "Loss after mini-batch  1000: 2.234\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.101\n",
      "Loss after mini-batch   200: 2.122\n",
      "Loss after mini-batch   300: 2.096\n",
      "Loss after mini-batch   400: 2.163\n",
      "Loss after mini-batch   500: 2.172\n",
      "Loss after mini-batch   600: 2.135\n",
      "Loss after mini-batch   700: 2.146\n",
      "Loss after mini-batch   800: 2.159\n",
      "Loss after mini-batch   900: 2.164\n",
      "Loss after mini-batch  1000: 2.206\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 1.992\n",
      "Loss after mini-batch   200: 2.017\n",
      "Loss after mini-batch   300: 2.067\n",
      "Loss after mini-batch   400: 2.034\n",
      "Loss after mini-batch   500: 2.030\n",
      "Loss after mini-batch   600: 2.068\n",
      "Loss after mini-batch   700: 2.077\n",
      "Loss after mini-batch   800: 2.079\n",
      "Loss after mini-batch   900: 2.098\n",
      "Loss after mini-batch  1000: 2.072\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.911\n",
      "Loss after mini-batch   200: 1.938\n",
      "Loss after mini-batch   300: 1.916\n",
      "Loss after mini-batch   400: 1.953\n",
      "Loss after mini-batch   500: 2.028\n",
      "Loss after mini-batch   600: 1.963\n",
      "Loss after mini-batch   700: 1.974\n",
      "Loss after mini-batch   800: 2.040\n",
      "Loss after mini-batch   900: 2.011\n",
      "Loss after mini-batch  1000: 2.062\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.796\n",
      "Loss after mini-batch   200: 1.831\n",
      "Loss after mini-batch   300: 1.852\n",
      "Loss after mini-batch   400: 1.893\n",
      "Loss after mini-batch   500: 1.875\n",
      "Loss after mini-batch   600: 1.924\n",
      "Loss after mini-batch   700: 1.890\n",
      "Loss after mini-batch   800: 1.968\n",
      "Loss after mini-batch   900: 1.941\n",
      "Loss after mini-batch  1000: 1.941\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.714\n",
      "Loss after mini-batch   200: 1.720\n",
      "Loss after mini-batch   300: 1.776\n",
      "Loss after mini-batch   400: 1.750\n",
      "Loss after mini-batch   500: 1.785\n",
      "Loss after mini-batch   600: 1.799\n",
      "Loss after mini-batch   700: 1.804\n",
      "Loss after mini-batch   800: 1.860\n",
      "Loss after mini-batch   900: 1.855\n",
      "Loss after mini-batch  1000: 1.921\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.625\n",
      "Loss after mini-batch   200: 1.619\n",
      "Loss after mini-batch   300: 1.621\n",
      "Loss after mini-batch   400: 1.657\n",
      "Loss after mini-batch   500: 1.743\n",
      "Loss after mini-batch   600: 1.747\n",
      "Loss after mini-batch   700: 1.753\n",
      "Loss after mini-batch   800: 1.760\n",
      "Loss after mini-batch   900: 1.779\n",
      "Loss after mini-batch  1000: 1.753\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.507\n",
      "Loss after mini-batch   200: 1.547\n",
      "Loss after mini-batch   300: 1.573\n",
      "Loss after mini-batch   400: 1.563\n",
      "Loss after mini-batch   500: 1.630\n",
      "Loss after mini-batch   600: 1.628\n",
      "Loss after mini-batch   700: 1.614\n",
      "Loss after mini-batch   800: 1.695\n",
      "Loss after mini-batch   900: 1.698\n",
      "Loss after mini-batch  1000: 1.753\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.429\n",
      "Loss after mini-batch   200: 1.476\n",
      "Loss after mini-batch   300: 1.487\n",
      "Loss after mini-batch   400: 1.491\n",
      "Loss after mini-batch   500: 1.478\n",
      "Loss after mini-batch   600: 1.542\n",
      "Loss after mini-batch   700: 1.561\n",
      "Loss after mini-batch   800: 1.582\n",
      "Loss after mini-batch   900: 1.585\n",
      "Loss after mini-batch  1000: 1.641\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.363\n",
      "Loss after mini-batch   200: 1.369\n",
      "Loss after mini-batch   300: 1.426\n",
      "Loss after mini-batch   400: 1.393\n",
      "Loss after mini-batch   500: 1.404\n",
      "Loss after mini-batch   600: 1.418\n",
      "Loss after mini-batch   700: 1.454\n",
      "Loss after mini-batch   800: 1.454\n",
      "Loss after mini-batch   900: 1.486\n",
      "Loss after mini-batch  1000: 1.492\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.303\n",
      "Loss after mini-batch   200: 1.264\n",
      "Loss after mini-batch   300: 1.284\n",
      "Loss after mini-batch   400: 1.272\n",
      "Loss after mini-batch   500: 1.332\n",
      "Loss after mini-batch   600: 1.352\n",
      "Loss after mini-batch   700: 1.379\n",
      "Loss after mini-batch   800: 1.398\n",
      "Loss after mini-batch   900: 1.422\n",
      "Loss after mini-batch  1000: 1.394\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.110\n",
      "Loss after mini-batch   200: 1.194\n",
      "Loss after mini-batch   300: 1.210\n",
      "Loss after mini-batch   400: 1.215\n",
      "Loss after mini-batch   500: 1.211\n",
      "Loss after mini-batch   600: 1.239\n",
      "Loss after mini-batch   700: 1.302\n",
      "Loss after mini-batch   800: 1.283\n",
      "Loss after mini-batch   900: 1.315\n",
      "Loss after mini-batch  1000: 1.315\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.074\n",
      "Loss after mini-batch   200: 1.047\n",
      "Loss after mini-batch   300: 1.099\n",
      "Loss after mini-batch   400: 1.119\n",
      "Loss after mini-batch   500: 1.180\n",
      "Loss after mini-batch   600: 1.177\n",
      "Loss after mini-batch   700: 1.167\n",
      "Loss after mini-batch   800: 1.186\n",
      "Loss after mini-batch   900: 1.194\n",
      "Loss after mini-batch  1000: 1.230\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 1.025\n",
      "Loss after mini-batch   200: 0.999\n",
      "Loss after mini-batch   300: 1.001\n",
      "Loss after mini-batch   400: 1.022\n",
      "Loss after mini-batch   500: 1.077\n",
      "Loss after mini-batch   600: 1.074\n",
      "Loss after mini-batch   700: 1.086\n",
      "Loss after mini-batch   800: 1.111\n",
      "Loss after mini-batch   900: 1.063\n",
      "Loss after mini-batch  1000: 1.155\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.887\n",
      "Loss after mini-batch   200: 0.928\n",
      "Loss after mini-batch   300: 0.925\n",
      "Loss after mini-batch   400: 0.928\n",
      "Loss after mini-batch   500: 0.981\n",
      "Loss after mini-batch   600: 1.019\n",
      "Loss after mini-batch   700: 0.989\n",
      "Loss after mini-batch   800: 0.995\n",
      "Loss after mini-batch   900: 1.026\n",
      "Loss after mini-batch  1000: 1.048\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.807\n",
      "Loss after mini-batch   200: 0.817\n",
      "Loss after mini-batch   300: 0.854\n",
      "Loss after mini-batch   400: 0.881\n",
      "Loss after mini-batch   500: 0.891\n",
      "Loss after mini-batch   600: 0.890\n",
      "Loss after mini-batch   700: 0.890\n",
      "Loss after mini-batch   800: 0.946\n",
      "Loss after mini-batch   900: 0.953\n",
      "Loss after mini-batch  1000: 0.978\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.747\n",
      "Loss after mini-batch   200: 0.787\n",
      "Loss after mini-batch   300: 0.776\n",
      "Loss after mini-batch   400: 0.815\n",
      "Loss after mini-batch   500: 0.776\n",
      "Loss after mini-batch   600: 0.848\n",
      "Loss after mini-batch   700: 0.856\n",
      "Loss after mini-batch   800: 0.870\n",
      "Loss after mini-batch   900: 0.850\n",
      "Loss after mini-batch  1000: 0.873\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.707\n",
      "Loss after mini-batch   200: 0.703\n",
      "Loss after mini-batch   300: 0.713\n",
      "Loss after mini-batch   400: 0.729\n",
      "Loss after mini-batch   500: 0.732\n",
      "Loss after mini-batch   600: 0.746\n",
      "Loss after mini-batch   700: 0.734\n",
      "Loss after mini-batch   800: 0.810\n",
      "Loss after mini-batch   900: 0.764\n",
      "Loss after mini-batch  1000: 0.808\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 4: 19 %\n",
      "--------------------------------\n",
      "687 3553\n",
      "targets 1 1\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.188\n",
      "Loss after mini-batch   200: 3.181\n",
      "Loss after mini-batch   300: 3.179\n",
      "Loss after mini-batch   400: 3.177\n",
      "Loss after mini-batch   500: 3.178\n",
      "Loss after mini-batch   600: 3.178\n",
      "Loss after mini-batch   700: 3.176\n",
      "Loss after mini-batch   800: 3.176\n",
      "Loss after mini-batch   900: 3.172\n",
      "Loss after mini-batch  1000: 3.172\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.170\n",
      "Loss after mini-batch   200: 3.169\n",
      "Loss after mini-batch   300: 3.165\n",
      "Loss after mini-batch   400: 3.165\n",
      "Loss after mini-batch   500: 3.160\n",
      "Loss after mini-batch   600: 3.161\n",
      "Loss after mini-batch   700: 3.154\n",
      "Loss after mini-batch   800: 3.156\n",
      "Loss after mini-batch   900: 3.151\n",
      "Loss after mini-batch  1000: 3.144\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.135\n",
      "Loss after mini-batch   200: 3.117\n",
      "Loss after mini-batch   300: 3.119\n",
      "Loss after mini-batch   400: 3.103\n",
      "Loss after mini-batch   500: 3.093\n",
      "Loss after mini-batch   600: 3.062\n",
      "Loss after mini-batch   700: 3.052\n",
      "Loss after mini-batch   800: 3.030\n",
      "Loss after mini-batch   900: 3.020\n",
      "Loss after mini-batch  1000: 2.957\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.953\n",
      "Loss after mini-batch   200: 2.913\n",
      "Loss after mini-batch   300: 2.895\n",
      "Loss after mini-batch   400: 2.885\n",
      "Loss after mini-batch   500: 2.858\n",
      "Loss after mini-batch   600: 2.843\n",
      "Loss after mini-batch   700: 2.850\n",
      "Loss after mini-batch   800: 2.836\n",
      "Loss after mini-batch   900: 2.803\n",
      "Loss after mini-batch  1000: 2.785\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.723\n",
      "Loss after mini-batch   200: 2.732\n",
      "Loss after mini-batch   300: 2.694\n",
      "Loss after mini-batch   400: 2.691\n",
      "Loss after mini-batch   500: 2.649\n",
      "Loss after mini-batch   600: 2.649\n",
      "Loss after mini-batch   700: 2.651\n",
      "Loss after mini-batch   800: 2.614\n",
      "Loss after mini-batch   900: 2.614\n",
      "Loss after mini-batch  1000: 2.580\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.520\n",
      "Loss after mini-batch   200: 2.506\n",
      "Loss after mini-batch   300: 2.517\n",
      "Loss after mini-batch   400: 2.496\n",
      "Loss after mini-batch   500: 2.454\n",
      "Loss after mini-batch   600: 2.459\n",
      "Loss after mini-batch   700: 2.473\n",
      "Loss after mini-batch   800: 2.488\n",
      "Loss after mini-batch   900: 2.450\n",
      "Loss after mini-batch  1000: 2.440\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.372\n",
      "Loss after mini-batch   200: 2.373\n",
      "Loss after mini-batch   300: 2.390\n",
      "Loss after mini-batch   400: 2.364\n",
      "Loss after mini-batch   500: 2.333\n",
      "Loss after mini-batch   600: 2.390\n",
      "Loss after mini-batch   700: 2.389\n",
      "Loss after mini-batch   800: 2.395\n",
      "Loss after mini-batch   900: 2.304\n",
      "Loss after mini-batch  1000: 2.345\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.261\n",
      "Loss after mini-batch   200: 2.241\n",
      "Loss after mini-batch   300: 2.270\n",
      "Loss after mini-batch   400: 2.259\n",
      "Loss after mini-batch   500: 2.249\n",
      "Loss after mini-batch   600: 2.256\n",
      "Loss after mini-batch   700: 2.269\n",
      "Loss after mini-batch   800: 2.292\n",
      "Loss after mini-batch   900: 2.289\n",
      "Loss after mini-batch  1000: 2.286\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.139\n",
      "Loss after mini-batch   200: 2.181\n",
      "Loss after mini-batch   300: 2.174\n",
      "Loss after mini-batch   400: 2.195\n",
      "Loss after mini-batch   500: 2.144\n",
      "Loss after mini-batch   600: 2.181\n",
      "Loss after mini-batch   700: 2.170\n",
      "Loss after mini-batch   800: 2.223\n",
      "Loss after mini-batch   900: 2.213\n",
      "Loss after mini-batch  1000: 2.174\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.058\n",
      "Loss after mini-batch   200: 2.050\n",
      "Loss after mini-batch   300: 2.062\n",
      "Loss after mini-batch   400: 2.080\n",
      "Loss after mini-batch   500: 2.094\n",
      "Loss after mini-batch   600: 2.118\n",
      "Loss after mini-batch   700: 2.079\n",
      "Loss after mini-batch   800: 2.090\n",
      "Loss after mini-batch   900: 2.094\n",
      "Loss after mini-batch  1000: 2.166\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.926\n",
      "Loss after mini-batch   200: 1.961\n",
      "Loss after mini-batch   300: 1.974\n",
      "Loss after mini-batch   400: 1.971\n",
      "Loss after mini-batch   500: 2.016\n",
      "Loss after mini-batch   600: 1.971\n",
      "Loss after mini-batch   700: 2.006\n",
      "Loss after mini-batch   800: 2.073\n",
      "Loss after mini-batch   900: 2.083\n",
      "Loss after mini-batch  1000: 2.041\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.816\n",
      "Loss after mini-batch   200: 1.850\n",
      "Loss after mini-batch   300: 1.876\n",
      "Loss after mini-batch   400: 1.894\n",
      "Loss after mini-batch   500: 1.893\n",
      "Loss after mini-batch   600: 1.909\n",
      "Loss after mini-batch   700: 1.930\n",
      "Loss after mini-batch   800: 1.922\n",
      "Loss after mini-batch   900: 1.989\n",
      "Loss after mini-batch  1000: 1.997\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.745\n",
      "Loss after mini-batch   200: 1.755\n",
      "Loss after mini-batch   300: 1.756\n",
      "Loss after mini-batch   400: 1.793\n",
      "Loss after mini-batch   500: 1.823\n",
      "Loss after mini-batch   600: 1.809\n",
      "Loss after mini-batch   700: 1.849\n",
      "Loss after mini-batch   800: 1.853\n",
      "Loss after mini-batch   900: 1.861\n",
      "Loss after mini-batch  1000: 1.882\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.627\n",
      "Loss after mini-batch   200: 1.676\n",
      "Loss after mini-batch   300: 1.692\n",
      "Loss after mini-batch   400: 1.675\n",
      "Loss after mini-batch   500: 1.668\n",
      "Loss after mini-batch   600: 1.761\n",
      "Loss after mini-batch   700: 1.743\n",
      "Loss after mini-batch   800: 1.773\n",
      "Loss after mini-batch   900: 1.763\n",
      "Loss after mini-batch  1000: 1.790\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.533\n",
      "Loss after mini-batch   200: 1.514\n",
      "Loss after mini-batch   300: 1.578\n",
      "Loss after mini-batch   400: 1.636\n",
      "Loss after mini-batch   500: 1.628\n",
      "Loss after mini-batch   600: 1.638\n",
      "Loss after mini-batch   700: 1.660\n",
      "Loss after mini-batch   800: 1.695\n",
      "Loss after mini-batch   900: 1.650\n",
      "Loss after mini-batch  1000: 1.718\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.427\n",
      "Loss after mini-batch   200: 1.446\n",
      "Loss after mini-batch   300: 1.484\n",
      "Loss after mini-batch   400: 1.528\n",
      "Loss after mini-batch   500: 1.530\n",
      "Loss after mini-batch   600: 1.517\n",
      "Loss after mini-batch   700: 1.604\n",
      "Loss after mini-batch   800: 1.559\n",
      "Loss after mini-batch   900: 1.580\n",
      "Loss after mini-batch  1000: 1.632\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.398\n",
      "Loss after mini-batch   200: 1.344\n",
      "Loss after mini-batch   300: 1.382\n",
      "Loss after mini-batch   400: 1.418\n",
      "Loss after mini-batch   500: 1.409\n",
      "Loss after mini-batch   600: 1.449\n",
      "Loss after mini-batch   700: 1.466\n",
      "Loss after mini-batch   800: 1.489\n",
      "Loss after mini-batch   900: 1.494\n",
      "Loss after mini-batch  1000: 1.511\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.261\n",
      "Loss after mini-batch   200: 1.261\n",
      "Loss after mini-batch   300: 1.259\n",
      "Loss after mini-batch   400: 1.289\n",
      "Loss after mini-batch   500: 1.317\n",
      "Loss after mini-batch   600: 1.344\n",
      "Loss after mini-batch   700: 1.400\n",
      "Loss after mini-batch   800: 1.397\n",
      "Loss after mini-batch   900: 1.404\n",
      "Loss after mini-batch  1000: 1.437\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.103\n",
      "Loss after mini-batch   200: 1.156\n",
      "Loss after mini-batch   300: 1.173\n",
      "Loss after mini-batch   400: 1.200\n",
      "Loss after mini-batch   500: 1.249\n",
      "Loss after mini-batch   600: 1.241\n",
      "Loss after mini-batch   700: 1.270\n",
      "Loss after mini-batch   800: 1.254\n",
      "Loss after mini-batch   900: 1.327\n",
      "Loss after mini-batch  1000: 1.315\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.086\n",
      "Loss after mini-batch   200: 1.108\n",
      "Loss after mini-batch   300: 1.084\n",
      "Loss after mini-batch   400: 1.155\n",
      "Loss after mini-batch   500: 1.103\n",
      "Loss after mini-batch   600: 1.172\n",
      "Loss after mini-batch   700: 1.167\n",
      "Loss after mini-batch   800: 1.196\n",
      "Loss after mini-batch   900: 1.223\n",
      "Loss after mini-batch  1000: 1.215\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.982\n",
      "Loss after mini-batch   200: 0.993\n",
      "Loss after mini-batch   300: 0.962\n",
      "Loss after mini-batch   400: 1.029\n",
      "Loss after mini-batch   500: 1.025\n",
      "Loss after mini-batch   600: 1.078\n",
      "Loss after mini-batch   700: 1.060\n",
      "Loss after mini-batch   800: 1.098\n",
      "Loss after mini-batch   900: 1.085\n",
      "Loss after mini-batch  1000: 1.166\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.886\n",
      "Loss after mini-batch   200: 0.888\n",
      "Loss after mini-batch   300: 0.918\n",
      "Loss after mini-batch   400: 0.943\n",
      "Loss after mini-batch   500: 0.974\n",
      "Loss after mini-batch   600: 0.954\n",
      "Loss after mini-batch   700: 0.990\n",
      "Loss after mini-batch   800: 1.025\n",
      "Loss after mini-batch   900: 0.998\n",
      "Loss after mini-batch  1000: 1.037\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.842\n",
      "Loss after mini-batch   200: 0.818\n",
      "Loss after mini-batch   300: 0.888\n",
      "Loss after mini-batch   400: 0.857\n",
      "Loss after mini-batch   500: 0.838\n",
      "Loss after mini-batch   600: 0.908\n",
      "Loss after mini-batch   700: 0.923\n",
      "Loss after mini-batch   800: 0.920\n",
      "Loss after mini-batch   900: 0.937\n",
      "Loss after mini-batch  1000: 0.955\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.762\n",
      "Loss after mini-batch   200: 0.758\n",
      "Loss after mini-batch   300: 0.768\n",
      "Loss after mini-batch   400: 0.791\n",
      "Loss after mini-batch   500: 0.779\n",
      "Loss after mini-batch   600: 0.794\n",
      "Loss after mini-batch   700: 0.867\n",
      "Loss after mini-batch   800: 0.850\n",
      "Loss after mini-batch   900: 0.849\n",
      "Loss after mini-batch  1000: 0.891\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.703\n",
      "Loss after mini-batch   200: 0.705\n",
      "Loss after mini-batch   300: 0.712\n",
      "Loss after mini-batch   400: 0.715\n",
      "Loss after mini-batch   500: 0.766\n",
      "Loss after mini-batch   600: 0.753\n",
      "Loss after mini-batch   700: 0.755\n",
      "Loss after mini-batch   800: 0.815\n",
      "Loss after mini-batch   900: 0.812\n",
      "Loss after mini-batch  1000: 0.803\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 5: 20 %\n",
      "--------------------------------\n",
      "724 3553\n",
      "targets 1 1\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.188\n",
      "Loss after mini-batch   200: 3.180\n",
      "Loss after mini-batch   300: 3.179\n",
      "Loss after mini-batch   400: 3.177\n",
      "Loss after mini-batch   500: 3.176\n",
      "Loss after mini-batch   600: 3.177\n",
      "Loss after mini-batch   700: 3.175\n",
      "Loss after mini-batch   800: 3.170\n",
      "Loss after mini-batch   900: 3.173\n",
      "Loss after mini-batch  1000: 3.172\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.168\n",
      "Loss after mini-batch   200: 3.163\n",
      "Loss after mini-batch   300: 3.162\n",
      "Loss after mini-batch   400: 3.159\n",
      "Loss after mini-batch   500: 3.153\n",
      "Loss after mini-batch   600: 3.150\n",
      "Loss after mini-batch   700: 3.138\n",
      "Loss after mini-batch   800: 3.134\n",
      "Loss after mini-batch   900: 3.125\n",
      "Loss after mini-batch  1000: 3.106\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.084\n",
      "Loss after mini-batch   200: 3.059\n",
      "Loss after mini-batch   300: 3.047\n",
      "Loss after mini-batch   400: 3.020\n",
      "Loss after mini-batch   500: 2.985\n",
      "Loss after mini-batch   600: 2.966\n",
      "Loss after mini-batch   700: 2.952\n",
      "Loss after mini-batch   800: 2.909\n",
      "Loss after mini-batch   900: 2.875\n",
      "Loss after mini-batch  1000: 2.856\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.795\n",
      "Loss after mini-batch   200: 2.804\n",
      "Loss after mini-batch   300: 2.781\n",
      "Loss after mini-batch   400: 2.759\n",
      "Loss after mini-batch   500: 2.729\n",
      "Loss after mini-batch   600: 2.702\n",
      "Loss after mini-batch   700: 2.692\n",
      "Loss after mini-batch   800: 2.662\n",
      "Loss after mini-batch   900: 2.650\n",
      "Loss after mini-batch  1000: 2.652\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.572\n",
      "Loss after mini-batch   200: 2.559\n",
      "Loss after mini-batch   300: 2.528\n",
      "Loss after mini-batch   400: 2.545\n",
      "Loss after mini-batch   500: 2.522\n",
      "Loss after mini-batch   600: 2.521\n",
      "Loss after mini-batch   700: 2.500\n",
      "Loss after mini-batch   800: 2.519\n",
      "Loss after mini-batch   900: 2.498\n",
      "Loss after mini-batch  1000: 2.458\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.403\n",
      "Loss after mini-batch   200: 2.400\n",
      "Loss after mini-batch   300: 2.417\n",
      "Loss after mini-batch   400: 2.365\n",
      "Loss after mini-batch   500: 2.408\n",
      "Loss after mini-batch   600: 2.401\n",
      "Loss after mini-batch   700: 2.441\n",
      "Loss after mini-batch   800: 2.382\n",
      "Loss after mini-batch   900: 2.373\n",
      "Loss after mini-batch  1000: 2.389\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.297\n",
      "Loss after mini-batch   200: 2.323\n",
      "Loss after mini-batch   300: 2.304\n",
      "Loss after mini-batch   400: 2.292\n",
      "Loss after mini-batch   500: 2.340\n",
      "Loss after mini-batch   600: 2.282\n",
      "Loss after mini-batch   700: 2.331\n",
      "Loss after mini-batch   800: 2.299\n",
      "Loss after mini-batch   900: 2.312\n",
      "Loss after mini-batch  1000: 2.282\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.208\n",
      "Loss after mini-batch   200: 2.214\n",
      "Loss after mini-batch   300: 2.180\n",
      "Loss after mini-batch   400: 2.193\n",
      "Loss after mini-batch   500: 2.248\n",
      "Loss after mini-batch   600: 2.221\n",
      "Loss after mini-batch   700: 2.180\n",
      "Loss after mini-batch   800: 2.210\n",
      "Loss after mini-batch   900: 2.246\n",
      "Loss after mini-batch  1000: 2.246\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.113\n",
      "Loss after mini-batch   200: 2.116\n",
      "Loss after mini-batch   300: 2.121\n",
      "Loss after mini-batch   400: 2.112\n",
      "Loss after mini-batch   500: 2.152\n",
      "Loss after mini-batch   600: 2.131\n",
      "Loss after mini-batch   700: 2.137\n",
      "Loss after mini-batch   800: 2.138\n",
      "Loss after mini-batch   900: 2.171\n",
      "Loss after mini-batch  1000: 2.108\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.041\n",
      "Loss after mini-batch   200: 1.971\n",
      "Loss after mini-batch   300: 2.011\n",
      "Loss after mini-batch   400: 2.042\n",
      "Loss after mini-batch   500: 2.039\n",
      "Loss after mini-batch   600: 2.100\n",
      "Loss after mini-batch   700: 2.080\n",
      "Loss after mini-batch   800: 2.085\n",
      "Loss after mini-batch   900: 2.069\n",
      "Loss after mini-batch  1000: 2.082\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.919\n",
      "Loss after mini-batch   200: 1.884\n",
      "Loss after mini-batch   300: 1.962\n",
      "Loss after mini-batch   400: 1.951\n",
      "Loss after mini-batch   500: 1.992\n",
      "Loss after mini-batch   600: 1.937\n",
      "Loss after mini-batch   700: 1.973\n",
      "Loss after mini-batch   800: 2.013\n",
      "Loss after mini-batch   900: 2.003\n",
      "Loss after mini-batch  1000: 2.020\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.798\n",
      "Loss after mini-batch   200: 1.827\n",
      "Loss after mini-batch   300: 1.885\n",
      "Loss after mini-batch   400: 1.885\n",
      "Loss after mini-batch   500: 1.876\n",
      "Loss after mini-batch   600: 1.908\n",
      "Loss after mini-batch   700: 1.898\n",
      "Loss after mini-batch   800: 1.931\n",
      "Loss after mini-batch   900: 1.918\n",
      "Loss after mini-batch  1000: 1.905\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.770\n",
      "Loss after mini-batch   200: 1.742\n",
      "Loss after mini-batch   300: 1.744\n",
      "Loss after mini-batch   400: 1.765\n",
      "Loss after mini-batch   500: 1.760\n",
      "Loss after mini-batch   600: 1.815\n",
      "Loss after mini-batch   700: 1.798\n",
      "Loss after mini-batch   800: 1.811\n",
      "Loss after mini-batch   900: 1.861\n",
      "Loss after mini-batch  1000: 1.837\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.595\n",
      "Loss after mini-batch   200: 1.630\n",
      "Loss after mini-batch   300: 1.681\n",
      "Loss after mini-batch   400: 1.697\n",
      "Loss after mini-batch   500: 1.709\n",
      "Loss after mini-batch   600: 1.691\n",
      "Loss after mini-batch   700: 1.753\n",
      "Loss after mini-batch   800: 1.718\n",
      "Loss after mini-batch   900: 1.756\n",
      "Loss after mini-batch  1000: 1.807\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.537\n",
      "Loss after mini-batch   200: 1.532\n",
      "Loss after mini-batch   300: 1.540\n",
      "Loss after mini-batch   400: 1.590\n",
      "Loss after mini-batch   500: 1.579\n",
      "Loss after mini-batch   600: 1.640\n",
      "Loss after mini-batch   700: 1.677\n",
      "Loss after mini-batch   800: 1.656\n",
      "Loss after mini-batch   900: 1.635\n",
      "Loss after mini-batch  1000: 1.684\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.425\n",
      "Loss after mini-batch   200: 1.436\n",
      "Loss after mini-batch   300: 1.470\n",
      "Loss after mini-batch   400: 1.458\n",
      "Loss after mini-batch   500: 1.482\n",
      "Loss after mini-batch   600: 1.561\n",
      "Loss after mini-batch   700: 1.555\n",
      "Loss after mini-batch   800: 1.586\n",
      "Loss after mini-batch   900: 1.578\n",
      "Loss after mini-batch  1000: 1.594\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.375\n",
      "Loss after mini-batch   200: 1.391\n",
      "Loss after mini-batch   300: 1.347\n",
      "Loss after mini-batch   400: 1.352\n",
      "Loss after mini-batch   500: 1.402\n",
      "Loss after mini-batch   600: 1.400\n",
      "Loss after mini-batch   700: 1.434\n",
      "Loss after mini-batch   800: 1.443\n",
      "Loss after mini-batch   900: 1.497\n",
      "Loss after mini-batch  1000: 1.476\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.218\n",
      "Loss after mini-batch   200: 1.265\n",
      "Loss after mini-batch   300: 1.262\n",
      "Loss after mini-batch   400: 1.332\n",
      "Loss after mini-batch   500: 1.314\n",
      "Loss after mini-batch   600: 1.342\n",
      "Loss after mini-batch   700: 1.357\n",
      "Loss after mini-batch   800: 1.348\n",
      "Loss after mini-batch   900: 1.376\n",
      "Loss after mini-batch  1000: 1.422\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.133\n",
      "Loss after mini-batch   200: 1.174\n",
      "Loss after mini-batch   300: 1.209\n",
      "Loss after mini-batch   400: 1.211\n",
      "Loss after mini-batch   500: 1.249\n",
      "Loss after mini-batch   600: 1.228\n",
      "Loss after mini-batch   700: 1.277\n",
      "Loss after mini-batch   800: 1.247\n",
      "Loss after mini-batch   900: 1.312\n",
      "Loss after mini-batch  1000: 1.271\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.056\n",
      "Loss after mini-batch   200: 1.063\n",
      "Loss after mini-batch   300: 1.077\n",
      "Loss after mini-batch   400: 1.104\n",
      "Loss after mini-batch   500: 1.144\n",
      "Loss after mini-batch   600: 1.132\n",
      "Loss after mini-batch   700: 1.161\n",
      "Loss after mini-batch   800: 1.202\n",
      "Loss after mini-batch   900: 1.203\n",
      "Loss after mini-batch  1000: 1.224\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.962\n",
      "Loss after mini-batch   200: 1.008\n",
      "Loss after mini-batch   300: 1.021\n",
      "Loss after mini-batch   400: 1.007\n",
      "Loss after mini-batch   500: 1.013\n",
      "Loss after mini-batch   600: 1.079\n",
      "Loss after mini-batch   700: 1.082\n",
      "Loss after mini-batch   800: 1.059\n",
      "Loss after mini-batch   900: 1.108\n",
      "Loss after mini-batch  1000: 1.131\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.881\n",
      "Loss after mini-batch   200: 0.892\n",
      "Loss after mini-batch   300: 0.909\n",
      "Loss after mini-batch   400: 0.958\n",
      "Loss after mini-batch   500: 0.951\n",
      "Loss after mini-batch   600: 1.011\n",
      "Loss after mini-batch   700: 0.968\n",
      "Loss after mini-batch   800: 1.021\n",
      "Loss after mini-batch   900: 0.938\n",
      "Loss after mini-batch  1000: 1.023\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.784\n",
      "Loss after mini-batch   200: 0.850\n",
      "Loss after mini-batch   300: 0.825\n",
      "Loss after mini-batch   400: 0.840\n",
      "Loss after mini-batch   500: 0.870\n",
      "Loss after mini-batch   600: 0.887\n",
      "Loss after mini-batch   700: 0.864\n",
      "Loss after mini-batch   800: 0.945\n",
      "Loss after mini-batch   900: 0.936\n",
      "Loss after mini-batch  1000: 0.924\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.734\n",
      "Loss after mini-batch   200: 0.743\n",
      "Loss after mini-batch   300: 0.745\n",
      "Loss after mini-batch   400: 0.761\n",
      "Loss after mini-batch   500: 0.783\n",
      "Loss after mini-batch   600: 0.789\n",
      "Loss after mini-batch   700: 0.848\n",
      "Loss after mini-batch   800: 0.826\n",
      "Loss after mini-batch   900: 0.869\n",
      "Loss after mini-batch  1000: 0.906\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.705\n",
      "Loss after mini-batch   200: 0.734\n",
      "Loss after mini-batch   300: 0.686\n",
      "Loss after mini-batch   400: 0.684\n",
      "Loss after mini-batch   500: 0.708\n",
      "Loss after mini-batch   600: 0.735\n",
      "Loss after mini-batch   700: 0.753\n",
      "Loss after mini-batch   800: 0.751\n",
      "Loss after mini-batch   900: 0.801\n",
      "Loss after mini-batch  1000: 0.779\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 6: 17 %\n",
      "--------------------------------\n",
      "637 3553\n",
      "targets 1 1\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.191\n",
      "Loss after mini-batch   200: 3.181\n",
      "Loss after mini-batch   300: 3.180\n",
      "Loss after mini-batch   400: 3.180\n",
      "Loss after mini-batch   500: 3.176\n",
      "Loss after mini-batch   600: 3.177\n",
      "Loss after mini-batch   700: 3.177\n",
      "Loss after mini-batch   800: 3.177\n",
      "Loss after mini-batch   900: 3.176\n",
      "Loss after mini-batch  1000: 3.173\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.171\n",
      "Loss after mini-batch   200: 3.170\n",
      "Loss after mini-batch   300: 3.169\n",
      "Loss after mini-batch   400: 3.167\n",
      "Loss after mini-batch   500: 3.164\n",
      "Loss after mini-batch   600: 3.160\n",
      "Loss after mini-batch   700: 3.160\n",
      "Loss after mini-batch   800: 3.157\n",
      "Loss after mini-batch   900: 3.153\n",
      "Loss after mini-batch  1000: 3.144\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   100: 3.136\n",
      "Loss after mini-batch   200: 3.126\n",
      "Loss after mini-batch   300: 3.109\n",
      "Loss after mini-batch   400: 3.100\n",
      "Loss after mini-batch   500: 3.082\n",
      "Loss after mini-batch   600: 3.044\n",
      "Loss after mini-batch   700: 3.015\n",
      "Loss after mini-batch   800: 2.974\n",
      "Loss after mini-batch   900: 2.943\n",
      "Loss after mini-batch  1000: 2.895\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   100: 2.833\n",
      "Loss after mini-batch   200: 2.787\n",
      "Loss after mini-batch   300: 2.771\n",
      "Loss after mini-batch   400: 2.710\n",
      "Loss after mini-batch   500: 2.688\n",
      "Loss after mini-batch   600: 2.660\n",
      "Loss after mini-batch   700: 2.662\n",
      "Loss after mini-batch   800: 2.626\n",
      "Loss after mini-batch   900: 2.617\n",
      "Loss after mini-batch  1000: 2.582\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   100: 2.529\n",
      "Loss after mini-batch   200: 2.528\n",
      "Loss after mini-batch   300: 2.509\n",
      "Loss after mini-batch   400: 2.465\n",
      "Loss after mini-batch   500: 2.515\n",
      "Loss after mini-batch   600: 2.500\n",
      "Loss after mini-batch   700: 2.471\n",
      "Loss after mini-batch   800: 2.482\n",
      "Loss after mini-batch   900: 2.481\n",
      "Loss after mini-batch  1000: 2.508\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   100: 2.440\n",
      "Loss after mini-batch   200: 2.414\n",
      "Loss after mini-batch   300: 2.375\n",
      "Loss after mini-batch   400: 2.397\n",
      "Loss after mini-batch   500: 2.430\n",
      "Loss after mini-batch   600: 2.413\n",
      "Loss after mini-batch   700: 2.417\n",
      "Loss after mini-batch   800: 2.372\n",
      "Loss after mini-batch   900: 2.389\n",
      "Loss after mini-batch  1000: 2.373\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   100: 2.308\n",
      "Loss after mini-batch   200: 2.320\n",
      "Loss after mini-batch   300: 2.279\n",
      "Loss after mini-batch   400: 2.363\n",
      "Loss after mini-batch   500: 2.319\n",
      "Loss after mini-batch   600: 2.316\n",
      "Loss after mini-batch   700: 2.326\n",
      "Loss after mini-batch   800: 2.325\n",
      "Loss after mini-batch   900: 2.302\n",
      "Loss after mini-batch  1000: 2.318\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   100: 2.218\n",
      "Loss after mini-batch   200: 2.204\n",
      "Loss after mini-batch   300: 2.201\n",
      "Loss after mini-batch   400: 2.242\n",
      "Loss after mini-batch   500: 2.253\n",
      "Loss after mini-batch   600: 2.242\n",
      "Loss after mini-batch   700: 2.189\n",
      "Loss after mini-batch   800: 2.234\n",
      "Loss after mini-batch   900: 2.258\n",
      "Loss after mini-batch  1000: 2.288\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   100: 2.123\n",
      "Loss after mini-batch   200: 2.103\n",
      "Loss after mini-batch   300: 2.141\n",
      "Loss after mini-batch   400: 2.165\n",
      "Loss after mini-batch   500: 2.171\n",
      "Loss after mini-batch   600: 2.186\n",
      "Loss after mini-batch   700: 2.158\n",
      "Loss after mini-batch   800: 2.182\n",
      "Loss after mini-batch   900: 2.152\n",
      "Loss after mini-batch  1000: 2.155\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   100: 2.019\n",
      "Loss after mini-batch   200: 2.011\n",
      "Loss after mini-batch   300: 2.051\n",
      "Loss after mini-batch   400: 2.053\n",
      "Loss after mini-batch   500: 2.049\n",
      "Loss after mini-batch   600: 2.077\n",
      "Loss after mini-batch   700: 2.071\n",
      "Loss after mini-batch   800: 2.114\n",
      "Loss after mini-batch   900: 2.129\n",
      "Loss after mini-batch  1000: 2.093\n",
      "Starting epoch 11\n",
      "Loss after mini-batch   100: 1.957\n",
      "Loss after mini-batch   200: 1.936\n",
      "Loss after mini-batch   300: 1.938\n",
      "Loss after mini-batch   400: 1.994\n",
      "Loss after mini-batch   500: 2.016\n",
      "Loss after mini-batch   600: 1.976\n",
      "Loss after mini-batch   700: 1.983\n",
      "Loss after mini-batch   800: 2.017\n",
      "Loss after mini-batch   900: 2.033\n",
      "Loss after mini-batch  1000: 2.003\n",
      "Starting epoch 12\n",
      "Loss after mini-batch   100: 1.812\n",
      "Loss after mini-batch   200: 1.842\n",
      "Loss after mini-batch   300: 1.880\n",
      "Loss after mini-batch   400: 1.865\n",
      "Loss after mini-batch   500: 1.858\n",
      "Loss after mini-batch   600: 1.937\n",
      "Loss after mini-batch   700: 1.905\n",
      "Loss after mini-batch   800: 1.965\n",
      "Loss after mini-batch   900: 1.954\n",
      "Loss after mini-batch  1000: 1.938\n",
      "Starting epoch 13\n",
      "Loss after mini-batch   100: 1.718\n",
      "Loss after mini-batch   200: 1.766\n",
      "Loss after mini-batch   300: 1.780\n",
      "Loss after mini-batch   400: 1.779\n",
      "Loss after mini-batch   500: 1.830\n",
      "Loss after mini-batch   600: 1.825\n",
      "Loss after mini-batch   700: 1.844\n",
      "Loss after mini-batch   800: 1.835\n",
      "Loss after mini-batch   900: 1.886\n",
      "Loss after mini-batch  1000: 1.837\n",
      "Starting epoch 14\n",
      "Loss after mini-batch   100: 1.667\n",
      "Loss after mini-batch   200: 1.643\n",
      "Loss after mini-batch   300: 1.674\n",
      "Loss after mini-batch   400: 1.677\n",
      "Loss after mini-batch   500: 1.705\n",
      "Loss after mini-batch   600: 1.709\n",
      "Loss after mini-batch   700: 1.746\n",
      "Loss after mini-batch   800: 1.803\n",
      "Loss after mini-batch   900: 1.781\n",
      "Loss after mini-batch  1000: 1.792\n",
      "Starting epoch 15\n",
      "Loss after mini-batch   100: 1.523\n",
      "Loss after mini-batch   200: 1.529\n",
      "Loss after mini-batch   300: 1.547\n",
      "Loss after mini-batch   400: 1.627\n",
      "Loss after mini-batch   500: 1.639\n",
      "Loss after mini-batch   600: 1.612\n",
      "Loss after mini-batch   700: 1.683\n",
      "Loss after mini-batch   800: 1.692\n",
      "Loss after mini-batch   900: 1.674\n",
      "Loss after mini-batch  1000: 1.675\n",
      "Starting epoch 16\n",
      "Loss after mini-batch   100: 1.435\n",
      "Loss after mini-batch   200: 1.448\n",
      "Loss after mini-batch   300: 1.471\n",
      "Loss after mini-batch   400: 1.494\n",
      "Loss after mini-batch   500: 1.492\n",
      "Loss after mini-batch   600: 1.541\n",
      "Loss after mini-batch   700: 1.572\n",
      "Loss after mini-batch   800: 1.584\n",
      "Loss after mini-batch   900: 1.563\n",
      "Loss after mini-batch  1000: 1.602\n",
      "Starting epoch 17\n",
      "Loss after mini-batch   100: 1.362\n",
      "Loss after mini-batch   200: 1.347\n",
      "Loss after mini-batch   300: 1.377\n",
      "Loss after mini-batch   400: 1.439\n",
      "Loss after mini-batch   500: 1.422\n",
      "Loss after mini-batch   600: 1.417\n",
      "Loss after mini-batch   700: 1.471\n",
      "Loss after mini-batch   800: 1.454\n",
      "Loss after mini-batch   900: 1.463\n",
      "Loss after mini-batch  1000: 1.487\n",
      "Starting epoch 18\n",
      "Loss after mini-batch   100: 1.256\n",
      "Loss after mini-batch   200: 1.211\n",
      "Loss after mini-batch   300: 1.260\n",
      "Loss after mini-batch   400: 1.314\n",
      "Loss after mini-batch   500: 1.306\n",
      "Loss after mini-batch   600: 1.327\n",
      "Loss after mini-batch   700: 1.371\n",
      "Loss after mini-batch   800: 1.362\n",
      "Loss after mini-batch   900: 1.337\n",
      "Loss after mini-batch  1000: 1.404\n",
      "Starting epoch 19\n",
      "Loss after mini-batch   100: 1.135\n",
      "Loss after mini-batch   200: 1.178\n",
      "Loss after mini-batch   300: 1.191\n",
      "Loss after mini-batch   400: 1.189\n",
      "Loss after mini-batch   500: 1.229\n",
      "Loss after mini-batch   600: 1.237\n",
      "Loss after mini-batch   700: 1.218\n",
      "Loss after mini-batch   800: 1.231\n",
      "Loss after mini-batch   900: 1.294\n",
      "Loss after mini-batch  1000: 1.285\n",
      "Starting epoch 20\n",
      "Loss after mini-batch   100: 1.021\n",
      "Loss after mini-batch   200: 1.056\n",
      "Loss after mini-batch   300: 1.085\n",
      "Loss after mini-batch   400: 1.073\n",
      "Loss after mini-batch   500: 1.139\n",
      "Loss after mini-batch   600: 1.134\n",
      "Loss after mini-batch   700: 1.166\n",
      "Loss after mini-batch   800: 1.102\n",
      "Loss after mini-batch   900: 1.213\n",
      "Loss after mini-batch  1000: 1.185\n",
      "Starting epoch 21\n",
      "Loss after mini-batch   100: 0.956\n",
      "Loss after mini-batch   200: 0.969\n",
      "Loss after mini-batch   300: 0.992\n",
      "Loss after mini-batch   400: 0.978\n",
      "Loss after mini-batch   500: 1.002\n",
      "Loss after mini-batch   600: 1.045\n",
      "Loss after mini-batch   700: 1.055\n",
      "Loss after mini-batch   800: 1.083\n",
      "Loss after mini-batch   900: 1.085\n",
      "Loss after mini-batch  1000: 1.083\n",
      "Starting epoch 22\n",
      "Loss after mini-batch   100: 0.853\n",
      "Loss after mini-batch   200: 0.853\n",
      "Loss after mini-batch   300: 0.894\n",
      "Loss after mini-batch   400: 0.912\n",
      "Loss after mini-batch   500: 0.921\n",
      "Loss after mini-batch   600: 0.897\n",
      "Loss after mini-batch   700: 0.958\n",
      "Loss after mini-batch   800: 0.962\n",
      "Loss after mini-batch   900: 1.028\n",
      "Loss after mini-batch  1000: 0.993\n",
      "Starting epoch 23\n",
      "Loss after mini-batch   100: 0.785\n",
      "Loss after mini-batch   200: 0.800\n",
      "Loss after mini-batch   300: 0.818\n",
      "Loss after mini-batch   400: 0.797\n",
      "Loss after mini-batch   500: 0.808\n",
      "Loss after mini-batch   600: 0.849\n",
      "Loss after mini-batch   700: 0.855\n",
      "Loss after mini-batch   800: 0.875\n",
      "Loss after mini-batch   900: 0.908\n",
      "Loss after mini-batch  1000: 0.924\n",
      "Starting epoch 24\n",
      "Loss after mini-batch   100: 0.703\n",
      "Loss after mini-batch   200: 0.710\n",
      "Loss after mini-batch   300: 0.758\n",
      "Loss after mini-batch   400: 0.760\n",
      "Loss after mini-batch   500: 0.739\n",
      "Loss after mini-batch   600: 0.757\n",
      "Loss after mini-batch   700: 0.762\n",
      "Loss after mini-batch   800: 0.774\n",
      "Loss after mini-batch   900: 0.792\n",
      "Loss after mini-batch  1000: 0.832\n",
      "Starting epoch 25\n",
      "Loss after mini-batch   100: 0.652\n",
      "Loss after mini-batch   200: 0.673\n",
      "Loss after mini-batch   300: 0.683\n",
      "Loss after mini-batch   400: 0.673\n",
      "Loss after mini-batch   500: 0.628\n",
      "Loss after mini-batch   600: 0.692\n",
      "Loss after mini-batch   700: 0.677\n",
      "Loss after mini-batch   800: 0.734\n",
      "Loss after mini-batch   900: 0.724\n",
      "Loss after mini-batch  1000: 0.751\n",
      "Training process has finished. Saving trained model.\n",
      "Starting testing\n",
      "Accuracy for fold 7: 20 %\n",
      "--------------------------------\n",
      "727 3553\n",
      "targets 1 1\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch   100: 3.191\n",
      "Loss after mini-batch   200: 3.179\n",
      "Loss after mini-batch   300: 3.180\n",
      "Loss after mini-batch   400: 3.178\n",
      "Loss after mini-batch   500: 3.176\n",
      "Loss after mini-batch   600: 3.175\n",
      "Loss after mini-batch   700: 3.174\n",
      "Loss after mini-batch   800: 3.176\n",
      "Loss after mini-batch   900: 3.173\n",
      "Loss after mini-batch  1000: 3.172\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   100: 3.169\n",
      "Loss after mini-batch   200: 3.166\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# For fold results\n",
    "results = {}\n",
    "  \n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "k_folds = 10\n",
    "# For fold results\n",
    "results = {}\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(training_data)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      training_data, \n",
    "                      batch_size=32, sampler=train_subsampler)\n",
    "    #print(trainloader)\n",
    "    #print(training_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      training_data,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "\n",
    "    # Init the neural network\n",
    "    network = Net().to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    #optimizer = torch.optim.Adam(network.parameters(), lr=1e-3, weight_decay = 0.0001)\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=0.0005, momentum = 0.9, weight_decay = 0.0001) \n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        #print(len(trainloader))\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            data[0] = data[0].to(device)\n",
    "            \n",
    "            data[0] = data[0] / 255.0\n",
    "            #print(data)\n",
    "            #print(\"________________\")\n",
    "            # Get inputs\n",
    "            \n",
    "            #inputs, targets = data\n",
    "            \n",
    "            \n",
    "            inputs = data[0].view(-1,1,105,600)\n",
    "            targets = data[1]\n",
    "            inputs = inputs.float()\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #print(inputs)\n",
    "            #for cnt, k in enumerate (inputs):\n",
    "                #print(k.shape)\n",
    "                #inputs[cnt] = k.view(-1,105,600)\n",
    "                #print(inputs[cnt].shape)\n",
    "                      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print(inputs.shape)\n",
    "            #inpouts = torch.unsqueeze(inputs, 0)\n",
    "            #print(inputs.view(10, 1, 105, 600).shape)\n",
    "    \n",
    "            # Perform forward pass\n",
    "            outputs = network(inputs)\n",
    "            \n",
    "            #outputs = torch.argmax(outputs)\n",
    "            #print(targets[0])\n",
    "            #targets = targets.long()\n",
    "            targets = targets.view(len(data[1]))\n",
    "            \n",
    "            #print(outputs.shape)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 100))\n",
    "                current_loss = 0.0\n",
    "            \n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "\n",
    "    # Saving the model\n",
    "    #save_path = f'./model-fold-{fold}.pth'\n",
    "    #torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            data[0] = data[0].to(device)\n",
    "            data[0] = data[0] / 255.0\n",
    "        # Get inputs\n",
    "            inputs = data[0].view(-1,1,105,600)\n",
    "            targets = data[1]\n",
    "            inputs = inputs.float()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Generate outputs\n",
    "            outputs = network(inputs)\n",
    "\n",
    "        # Set total and correct\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            #correct += (predicted == targets).sum().item()\n",
    "            cntr = 0\n",
    "            for i in predicted:\n",
    "                \n",
    "                if i == targets[cntr]:\n",
    "                    correct += 1\n",
    "                cntr += 1\n",
    "            #print((predicted == targets))\n",
    "            #print((predicted == targets).sum())\n",
    "            #print((predicted == targets).sum().item())\n",
    "            #print((predicted == targets).sum().item())\n",
    "            #print(total, correct)\n",
    "            #print(predicted , targets)\n",
    "\n",
    "      # Print accuracy\n",
    "        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "        print('--------------------------------')\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "        print (correct, total)\n",
    "        print(\"targets\", targets.size(0) , len(targets))\n",
    "\n",
    "  # Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "print(f'Average: {sum/len(results.items())} %')\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(None, dtype=object), tensor([2])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
