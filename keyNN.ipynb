{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "key_training_data = []\n",
    "\n",
    "class Labeling():        \n",
    "        \n",
    "    def make_training_data():\n",
    "        \n",
    "        \n",
    "        keys = ['C','C#','Db','D','D#','Eb','E','F','F#','Gb','G','G#','Ab','A','A#','Bb','B']\n",
    "        keyIndex = [0, 1, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 10, 11]\n",
    "        keys_counters= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        path = \"C:/outs/spects4096\"\n",
    "        \n",
    "        os.chdir(path)\n",
    "        spectrograms = os.listdir()\n",
    "        key_count = [0] * 24\n",
    "        annotations = open(\"C:/outs/shiftedAnnotations4096.txt\", \"r\")\n",
    "        #annotations = open(\"C:/outs/square4096.txt\", \"r\")\n",
    "        while True: \n",
    "            line = annotations.readline()\n",
    "            if not line:\n",
    "                break;\n",
    "            \n",
    "            \n",
    "            file, key, chord = line.split(' ', 2)\n",
    "            file = file.strip()\n",
    "            key = key.strip()\n",
    "            chord = chord.strip()\n",
    "            \n",
    "            file = file + \".LOFI.png\"\n",
    "            spect_path = os.path.join(path, file)\n",
    "            spect_path = cv2.imread(spect_path)\n",
    "           \n",
    "            if spect_path is None :\n",
    "                continue;\n",
    "                \n",
    "                \n",
    "            for j,i in enumerate(keys):\n",
    "                \n",
    "                temp = keyIndex[j]\n",
    "                if keys_counters[temp] > 9 : \n",
    "                    continue;\n",
    "                \n",
    "                if key == i:\n",
    "                    key_training_data.append([np.array(spect_path)/255, torch.LongTensor([keyIndex[j]])])\n",
    "                    keys_counters[temp] += 1\n",
    "           \n",
    "        \n",
    "        \n",
    "        np.random.shuffle(key_training_data)\n",
    "        #print(keys_counters)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Labeling.make_training_data()\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0.87843137, 0.87843137, 0.87843137],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        ...,\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [0.58823529, 0.58823529, 0.58823529]],\n",
      "\n",
      "       [[0.87843137, 0.87843137, 0.87843137],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        ...,\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [0.58823529, 0.58823529, 0.58823529]],\n",
      "\n",
      "       [[0.87843137, 0.87843137, 0.87843137],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        ...,\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [1.        , 1.        , 1.        ],\n",
      "        [0.58431373, 0.58431373, 0.58431373]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.16078431, 0.16078431, 0.16078431],\n",
      "        [0.07843137, 0.07843137, 0.07843137],\n",
      "        [0.10196078, 0.10196078, 0.10196078],\n",
      "        ...,\n",
      "        [0.11372549, 0.11372549, 0.11372549],\n",
      "        [0.05490196, 0.05490196, 0.05490196],\n",
      "        [0.14901961, 0.14901961, 0.14901961]],\n",
      "\n",
      "       [[0.10196078, 0.10196078, 0.10196078],\n",
      "        [0.03529412, 0.03529412, 0.03529412],\n",
      "        [0.06666667, 0.06666667, 0.06666667],\n",
      "        ...,\n",
      "        [0.09803922, 0.09803922, 0.09803922],\n",
      "        [0.07058824, 0.07058824, 0.07058824],\n",
      "        [0.05490196, 0.05490196, 0.05490196]],\n",
      "\n",
      "       [[0.07843137, 0.07843137, 0.07843137],\n",
      "        [0.01176471, 0.01176471, 0.01176471],\n",
      "        [0.01568627, 0.01568627, 0.01568627],\n",
      "        ...,\n",
      "        [0.11764706, 0.11764706, 0.11764706],\n",
      "        [0.05098039, 0.05098039, 0.05098039],\n",
      "        [0.07843137, 0.07843137, 0.07843137]]]), tensor([4])]\n"
     ]
    }
   ],
   "source": [
    "print (key_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY ESTIMATION NEURAL NETWORK\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv4 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv5 = nn.Conv2d(8, 8, 5)\n",
    "\n",
    "        \n",
    "        x = torch.randn(3,105,600).view(-1,3,105,600)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.avg_pool2d(F.elu(self.conv5(x)), (2,2))\n",
    "        \n",
    "                \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    k_Net = Net().to(device)\n",
    "\n",
    "\n",
    "print(k_Net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING OPTIMIZER\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(k_Net.parameters(), lr= lr , momentum = 0.9, weight_decay = 0.0001) \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "print(\"optimizer komple \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpfull functions for training \n",
    "\n",
    "\n",
    "def lr_handler(loss, change_again):\n",
    "    \"\"\"an to loss exei pesei katw apo mia timh tote allazoume to lr\"\"\"\n",
    "    \n",
    "    if (loss < 0.7) & (change_again) :\n",
    "         \n",
    "        lr = 0.0005\n",
    "        optimizer = torch.optim.SGD(k_Net.parameters(), lr = lr, momentum = 0.9, weight_decay = 0.0001) \n",
    "        print(f\"\\nlr changed cause of very low loss: {loss} \\nNew lr: {lr} \\n\" )\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        change_again = False\n",
    "        \n",
    "    return change_again\n",
    "        \n",
    "def train_evaluation (loader, epochs):\n",
    "    \"\"\"trexoume ena mini test ana 5 epochs\"\"\"\n",
    "    acc = 101\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    if (((epochs+1) % 5) == 0) :\n",
    "\n",
    "        k_Net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, test_data in enumerate(loader, 0):\n",
    "                # prepare testing data \n",
    "                test_inputs = test_data[0].view(-1,3,105,600)\n",
    "                test_targets = test_data[1]\n",
    "                test_inputs = test_inputs.float()\n",
    "                \n",
    "                # Set data to GPU\n",
    "                test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "                # Generate outputs\n",
    "                test_outputs = k_Net(test_inputs)\n",
    "\n",
    "                # Set total and correct\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                total += test_targets.size(0)\n",
    "                \n",
    "                cntr = 0\n",
    "                for i in predicted:\n",
    "                    if i == test_targets[cntr]:\n",
    "                        correct += 1\n",
    "                    cntr += 1\n",
    "\n",
    "                    \n",
    "        acc = (correct / total)*100\n",
    "\n",
    "    k_Net.train()\n",
    "    return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET DATA DROM DISK key_1500_training_data.npy \n",
    "import numpy as np\n",
    "\n",
    "key_training_data = np.load(\"C:/outs/spects4096/all_key_training_data.npy\", allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mia fora 8a trexei auto. Metratoph se tensor kai apo8hkeush tou se tensor file \n",
    "\n",
    "#for i, data in enumerate(key_training_data): #ΑYTO EDW 8A STA KANEI TENSOR TA DATA SOU ALLA KANTA KAI ENA SAVE TO TENSOR https://discuss.pytorch.org/t/save-a-tensor-to-file/37136\n",
    "    #key_training_data[0] = torch.from_numpy(data[0])\n",
    "    #print(type(data[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(key_training_data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_size = int(len(key_training_data)*0.1)\n",
    "training_data = key_training_data[:len(key_training_data)-val_size]\n",
    "test_data = key_training_data[-val_size:]\n",
    "\n",
    "batchSize = 16\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchSize , shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batchSize, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING KEY NEURAL NETWORK\n",
    "\n",
    "\n",
    "k_Net.train()\n",
    "num_epochs = 20\n",
    "acc = 101 \n",
    "our_acc = 0\n",
    "flag = True # flag for not changing again and again the lr when loss remains below 0.5 \n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "        \n",
    "        # Set total and correct\n",
    "        targets = targets.view(len(data[1]))\n",
    "        \n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    flag = lr_handler(loss, flag) # check learning rate\n",
    "    acc = train_evaluation(test_dataloader, epoch) # evaluate the model every epoch \n",
    "    \n",
    "    if acc != 101:\n",
    "        \n",
    "        print(\"| Epoch : %2d | Loss: %.5f | currenct accuracy: %2.5f |\" % (epoch, loss, acc))\n",
    "        if (our_acc < acc): # if the accuracy imporves save the models parameteres\n",
    "            our_acc = acc\n",
    "            torch.save(k_Net.state_dict(), 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/test1-best-key-network-model-parameters1.pt')\n",
    "        acc = 101\n",
    "    else:\n",
    "        print(\"| Epoch : %2d | Loss: %.5f | \" % (epoch ,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(k_Net.state_dict(), 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-key-network-model-parameters-overfitted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING KEY NEURAL NETWORK\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 6\n",
    "test_data = key_training_data[-val_size:]\n",
    "test_dataloader = DataLoader(test_data, batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing ```\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"kaispera\")\n",
    "    \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        \n",
    "        cntr = 0\n",
    "        for i in predicted:# counter for correct predictions\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        \n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct)\n",
    "print(total)\n",
    "print(our_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING ON TRAINING DATA GIA NA TSEKAROUME OVERFITTING\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing overfit ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n",
    "#plt.plot(loss_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION AT SMALL DATA TRAINED MODEL \n",
    "\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/288-best-key-network-model-parameters.pt'))\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(key_training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_model = k_Net\n",
    "K_model.load_state_dict(torch.load(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-key-network-model-parameters-overfitted.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#TESTING ON TRAINING DATA GIA NA TSEKAROUME OVERFITTING\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(key_training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing overfit ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n",
    "#plt.plot(loss_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
