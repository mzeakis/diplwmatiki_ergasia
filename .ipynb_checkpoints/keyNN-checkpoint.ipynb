{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "key_training_data = []\n",
    "\n",
    "class Labeling():        \n",
    "        \n",
    "    def make_training_data():\n",
    "        \n",
    "        \n",
    "        keys = ['C','C#','Db','D','D#','Eb','E','F','F#','Gb','G','G#','Ab','A','A#','Bb','B']\n",
    "        keyIndex = [0, 1, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 10, 11]\n",
    "        keys_counters= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        path = \"C:/outs/spects4096\"\n",
    "        #path = \"C:/outs/square4096\"\n",
    "        #path = \"C:/outs/our_spects\"\n",
    "        os.chdir(path)\n",
    "        spectrograms = os.listdir()\n",
    "        key_count = [0] * 24\n",
    "        annotations = open(\"C:/outs/shiftedAnnotations4096.txt\", \"r\")\n",
    "        #annotations = open(\"C:/outs/square4096.txt\", \"r\")\n",
    "        while True: \n",
    "            line = annotations.readline()\n",
    "            if not line:\n",
    "                break;\n",
    "            \n",
    "            \n",
    "            file, key, chord = line.split(' ', 2)\n",
    "            file = file.strip()\n",
    "            key = key.strip()\n",
    "            chord = chord.strip()\n",
    "            #print(file)\n",
    "            file = file + \".LOFI.png\"\n",
    "            spect_path = os.path.join(path, file)\n",
    "            spect_path = cv2.imread(spect_path)\n",
    "            #spect_path = cv2.imread(spect_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            #print(spect_path)\n",
    "            if spect_path is None :\n",
    "                continue;\n",
    "                \n",
    "                \n",
    "            for j,i in enumerate(keys):\n",
    "                \n",
    "                temp = keyIndex[j]\n",
    "                if keys_counters[temp] > 999 : \n",
    "                    continue;\n",
    "                \n",
    "                if key == i:\n",
    "                    key_training_data.append([np.array(spect_path)/255, torch.LongTensor([keyIndex[j]])])\n",
    "                    keys_counters[temp] += 1\n",
    "           \n",
    "            #print(file)\n",
    "            #for j,i in enumerate(chords):\n",
    "                #if (chordsCounters[0] > 6500) & (chord == 'minor'):\n",
    "                    #continue;\n",
    "                    \n",
    "                #if (chordsCounters[1] > 2500) :\n",
    "                    #continue;\n",
    "                    \n",
    "                #if chord == i:\n",
    "                   #chord_training_data.append([np.array(spect_path)/255, torch.LongTensor([chordsIndex[j]])])\n",
    "                   # chordsCounters[j] += 1\n",
    "\n",
    "        \n",
    "        #np.random.shuffle(key_training_data)\n",
    "        np.random.shuffle(key_training_data)\n",
    "        #print(\"saving data......\\n\")\n",
    "        #np.save(\"1000-test_training_data.npy\", key_training_data)\n",
    "            \n",
    "        print(keys_counters)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Labeling.make_training_data()\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "tsifsa\n"
     ]
    }
   ],
   "source": [
    "print(len(key_training_data))\n",
    "print(\"tsifsa\")\n",
    "#print (key_training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv5): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=97440, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=12, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#KEY ESTIMATION NEURAL NETWORK\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv4 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv5 = nn.Conv2d(8, 8, 5)\n",
    "        #self.conv6 = nn.Conv2d(8, 8, 3)\n",
    "        #self.conv7 = nn.Conv2d(8, 8, 3)\n",
    "\n",
    "        \n",
    "        x = torch.randn(3,105,600).view(-1,3,105,600)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv1(x)), (2,2))\n",
    "        \n",
    "        x = F.elu(self.conv2(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv2(x)), (2,2))\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv3(x)), (2,2))\n",
    "        \n",
    "        x = F.elu(self.conv4(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv4(x)), (2,2))\n",
    "        \n",
    "        #x = F.elu(self.conv5(x))\n",
    "        x = F.avg_pool2d(F.elu(self.conv5(x)), (2,2))\n",
    "        \n",
    "        #x = F.elu(self.conv6(x))\n",
    "        #x = F.avg_pool2d(F.elu(self.conv6(x)), (2,2))\n",
    "\n",
    "        #x = F.avg_pool2d(F.elu(self.conv7(x)), (2,2))\n",
    "\n",
    "                \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "            #print(x.shape)\n",
    "            #print(self._to_linear)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    k_Net = Net().to(device)\n",
    "\n",
    "\n",
    "print(k_Net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer komple \n"
     ]
    }
   ],
   "source": [
    "#SETTING OPTIMIZER\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(k_Net.parameters(), lr= lr , momentum = 0.9, weight_decay = 0.0001) \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    loss_function = loss_function.to(device)\n",
    "\n",
    "print(\"optimizer komple \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpfull functions for training \n",
    "\n",
    "\n",
    "def lr_handler(loss, change_again):\n",
    "    \"\"\"an to loss exei pesei katw apo mia timh tote allazoume to lr\"\"\"\n",
    "    \n",
    "    if (loss < 0.0) & (change_again) :\n",
    "         \n",
    "        lr = 0.001\n",
    "        optimizer = torch.optim.SGD(k_Net.parameters(), lr = lr, momentum = 0.9, weight_decay = 0.0001) \n",
    "        print(f\"\\nlr changed cause of very low loss: {loss} \\nNew lr: {lr} \\n\" )\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        change_again = False\n",
    "        \n",
    "    return change_again\n",
    "        \n",
    "def train_evaluation (loader, epochs):\n",
    "    \"\"\"trexoume ena mini test ana 1 epochs\"\"\"\n",
    "    acc = 101\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    if (((epochs+1) % 1) == 0) :\n",
    "\n",
    "        k_Net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, test_data in enumerate(loader, 0):\n",
    "                # prepare testing data \n",
    "                test_inputs = test_data[0].view(-1,3,105,600)\n",
    "                test_targets = test_data[1]\n",
    "                test_inputs = test_inputs.float()\n",
    "                \n",
    "                # Set data to GPU\n",
    "                test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "                # Generate outputs\n",
    "                test_outputs = k_Net(test_inputs)\n",
    "\n",
    "                # Set total and correct\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                total += test_targets.size(0)\n",
    "                \n",
    "                cntr = 0\n",
    "                for i in predicted:\n",
    "                    if i == test_targets[cntr]:\n",
    "                        correct += 1\n",
    "                    cntr += 1\n",
    "\n",
    "                    \n",
    "        acc = (correct / total)*100\n",
    "\n",
    "    k_Net.train()\n",
    "    return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET DATA DROM DISK key_1500_training_data.npy \n",
    "import numpy as np\n",
    "\n",
    "key_training_data = np.load(\"C:/outs/spects4096/all_key_training_data.npy\", allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mia fora 8a trexei auto. Metratoph se tensor kai apo8hkeush tou se tensor file \n",
    "\n",
    "\n",
    "for i, data in enumerate(key_training_data): #ΑYTO EDW 8A STA KANEI TENSOR TA DATA SOU ALLA KANTA KAI ENA SAVE TO TENSOR https://discuss.pytorch.org/t/save-a-tensor-to-file/37136\n",
    "    key_training_data[0] = torch.from_numpy(data[0])\n",
    "    #print(type(data[0]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA \n",
    "\n",
    "\n",
    "val_size = int(len(key_training_data)*0.1) # set the 10% of training data to split it for evaluation \n",
    "\n",
    "\n",
    "training_data = key_training_data[:len(key_training_data)-val_size]\n",
    "\n",
    "test_data = key_training_data[-val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(key_training_data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_size = int(len(key_training_data)*0.1)\n",
    "training_data = key_training_data[:len(key_training_data)-val_size]\n",
    "test_data = key_training_data[-val_size:]\n",
    "\n",
    "batchSize = 16\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchSize , shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batchSize, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| Epoch :  0 | Loss: nan | currenct accuracy: 9.08333 |\n",
      "-----------------------------------------------------------\n",
      "| Epoch :  1 | Loss: nan | currenct accuracy: 9.08333 |\n",
      "-----------------------------------------------------------\n",
      "| Epoch :  2 | Loss: nan | currenct accuracy: 9.08333 |\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7a0d46a57262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING KEY NEURAL NETWORK\n",
    "\n",
    "\n",
    "k_Net.train()\n",
    "num_epochs = 15\n",
    "acc = 101 \n",
    "our_acc = 0\n",
    "flag = True # flag for not changing again and again the lr when loss remains below 0.5 \n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "        \n",
    "        # Set total and correct\n",
    "        targets = targets.view(len(data[1]))\n",
    "        \n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    flag = lr_handler(loss, flag) # check learning rate\n",
    "    acc = train_evaluation(test_dataloader, epoch) # evaluate the model every epoch \n",
    "    \n",
    "    if acc != 101:\n",
    "        \n",
    "        print(\"| Epoch : %2d | Loss: %.5f | currenct accuracy: %2.5f |\" % (epoch, loss, acc))\n",
    "        if (our_acc < acc): # if the accurace imporves save the models parameteres\n",
    "            our_acc = acc\n",
    "            torch.save(k_Net.state_dict(), 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/batchSize1-best-key-network-model-parameters1.pt')\n",
    "        acc = 101\n",
    "    else:\n",
    "        print(\"| Epoch : %2d | Loss: %.5f | \" % (epoch ,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(k_Net.state_dict(), 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-key-network-model-parameters-overfitted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` Starting Testing ```\n",
      "kaispera\n",
      "Accuracy:  55.486111111111114\n"
     ]
    }
   ],
   "source": [
    "#TESTING KEY NEURAL NETWORK\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 6\n",
    "test_data = key_training_data[-val_size:]\n",
    "test_dataloader = DataLoader(test_data, batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing ```\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"kaispera\")\n",
    "    \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        #print(\"kaispera 12 \")\n",
    "\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += targets.size(0)\n",
    "        \n",
    "        cntr = 0\n",
    "        for i in predicted:# counter for correct predictions\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        \n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "1440\n",
      "58.333333333333336\n"
     ]
    }
   ],
   "source": [
    "print(correct)\n",
    "print(total)\n",
    "print(our_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` Starting Testing overfit ```\n",
      "Accuracy:  94.95370370370371\n"
     ]
    }
   ],
   "source": [
    "#TESTING ON TRAINING DATA GIA NA TSEKAROUME OVERFITTING\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing overfit ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n",
    "#plt.plot(loss_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12306\n",
      "12960\n"
     ]
    }
   ],
   "source": [
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` Starting Testing ```\n",
      "Accuracy:  33.141666666666666\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION AT SMALL DATA TRAINED MODEL \n",
    "\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/288-best-key-network-model-parameters.pt'))\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(key_training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_model = k_Net\n",
    "K_model.load_state_dict(torch.load(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-key-network-model-parameters-overfitted.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` Starting Testing overfit ```\n",
      "Accuracy:  94.95833333333333\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#TESTING ON TRAINING DATA GIA NA TSEKAROUME OVERFITTING\n",
    "\n",
    "\n",
    "correct, total = 0, 0\n",
    "batchSize = 32\n",
    "overfit = DataLoader(key_training_data , batch_size = batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "k_Net.eval()\n",
    "\n",
    "print(\"``` Starting Testing overfit ```\")\n",
    "with torch.no_grad():\n",
    "    #running_loss = 0.0\n",
    "    #loss_values = [] \n",
    "\n",
    "    # Iterate over the test data and generate predictions\n",
    "    for i, data in enumerate(overfit, 0):\n",
    "    # Get inputs\n",
    "        inputs = data[0].view(-1,3,105,600)\n",
    "        targets = data[1]\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # Generate outputs\n",
    "        outputs = k_Net(inputs)\n",
    "\n",
    "    # Set total and correct\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        #running_loss =+ loss.item() * batchSize\n",
    "        #correct += (predicted == targets).sum().item()\n",
    "        cntr = 0\n",
    "        for i in predicted:\n",
    "\n",
    "            if i == targets[cntr]:\n",
    "                correct += 1\n",
    "            cntr += 1\n",
    "\n",
    "        #loss_values.append(running_loss / len(test_data))\n",
    "    \n",
    "print(\"Accuracy: \", 100*(correct/total))\n",
    "\n",
    "#plt.plot(loss_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
