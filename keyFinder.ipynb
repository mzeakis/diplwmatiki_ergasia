{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv5): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=97440, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=12, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#KEY ESTIMATION NEURAL NETWORK\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv4 = nn.Conv2d(8, 8, 5)\n",
    "        self.conv5 = nn.Conv2d(8, 8, 5)\n",
    "        \n",
    "\n",
    "        \n",
    "        x = torch.randn(3,105,600).view(-1,3,105,600) # giving a random input to find the input of the first linear layer \n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def convs(self, x):\n",
    "        \n",
    "        x = F.elu(self.conv1(x))        \n",
    "        x = F.elu(self.conv2(x))        \n",
    "        x = F.elu(self.conv3(x))        \n",
    "        x = F.elu(self.conv4(x))        \n",
    "        x = F.avg_pool2d(F.elu(self.conv5(x)), (2,2))\n",
    "\n",
    "                \n",
    "        if self._to_linear is None: #we have to define the input of the first linear layer \n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "if torch.cuda.is_available(): # set the variables at GPU \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    k_Net = Net().to(device)\n",
    "\n",
    "\n",
    "print(k_Net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chordNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=227328, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#CHORD ESTIMATION NEURAL NETWORK (binary classification)\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class chordNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,5)\n",
    "        self.conv2 = nn.Conv2d(16,16,5)\n",
    "        \n",
    "        x = torch.randn(105,600).view(-1,1,105,600) # giving a random input to find the input of the first linear layer \n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 8)\n",
    "        self.fc2 = nn.Linear(8, 2)\n",
    "        self.dropout = nn.Dropout(0.5)        \n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.avg_pool2d(F.elu(self.conv2(x)), (2,2))\n",
    "        \n",
    "        \n",
    "        if self._to_linear is None: # we have to define the input of the first linear layer \n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    c_Net = chordNet().to(device)\n",
    "\n",
    "\n",
    "print(c_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to convert WAV to spectrogram \n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import os\n",
    "import numpy\n",
    "\n",
    "\n",
    "# scale the values from 0 to 1 \n",
    "\n",
    "def scale_minmax(X, min=0.0, max=1.0): \n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "# making the spectrogram\n",
    "\n",
    "def spectrogram_image(y, sr, out, hop_length, n_mels): \n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                            n_fft=hop_length*2, hop_length=hop_length)\n",
    "    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(np.uint8)\n",
    "    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    skimage.io.imsave(out, img)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EKTELESIMO PAROUSIASHS !!!\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import os \n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "song = \"nothing\"\n",
    "keys = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "\n",
    "\n",
    "C_model = c_Net\n",
    "C_model.load_state_dict(torch.load(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-chord-network-model-parameters.pt\"))\n",
    "C_model.eval()\n",
    "\n",
    "K_model = k_Net\n",
    "K_model.load_state_dict(torch.load(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/saved_nns/best-key-network-model-parameters.pt\"))\n",
    "K_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "path = \"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/final_dataset_mp3\"\n",
    "os.chdir(path)\n",
    "mp3_files = os.listdir()\n",
    "\n",
    "while (song != \"0\"):\n",
    "    found = 0\n",
    "    \n",
    "    print(\"\\nSelect a song from the list:\")\n",
    "    \n",
    "    if (song == \"nothing\"): #print all the songs from our list \n",
    "        for i,file in enumerate(mp3_files):\n",
    "            name, ext = os.path.splitext(file)\n",
    "            print(f\"{i+1} {name}\")\n",
    "            if(i==41):\n",
    "                print(\"\\n\")\n",
    "    \n",
    "    song = input(\"\") # read from keyboard \n",
    "    \n",
    "    for file in mp3_files : #  search for the song\n",
    "        name, ext = os.path.splitext(file)\n",
    "        \n",
    "        if (name == song):\n",
    "            my_mp3 = file\n",
    "            found = 1\n",
    "            break;\n",
    "\n",
    "\n",
    "    #if name of the song exists \n",
    "    if (found == 1):\n",
    "        \n",
    "        #converting to WAV file \n",
    "        \n",
    "        print(\"\\nConverting mp3 to WAV...\")\n",
    "        pydub.AudioSegment.converter = r\"C:\\ffmpeg\\bin\\ffmpeg.exe\"\n",
    "        mp3_sound = AudioSegment.from_mp3(my_mp3)\n",
    "        #rename them using the old name + \".wav\"\n",
    "        mp3_sound.export(\"C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/test_dataset_wav/{0}.wav\".format(name), format=\"wav\")\n",
    "        \n",
    "        \n",
    "        print(\"-- mp3 to WAV convertion accomplished --\")\n",
    "        \n",
    "        #converting WAV to spectrogram\n",
    "        print(\"\\nConverting WAV to spectrogram...\")\n",
    "        hop_length = 4096 # number of samples per time-step in spectrogram\n",
    "        n_mels = 105 # number of bins in spectrogram. Height of image\n",
    "        time_steps = 599 # number of time-steps. Width of image\n",
    "\n",
    "        # extract a fixed length window\n",
    "        start_sample = 0 # starting at beginning\n",
    "        length_samples = time_steps*hop_length\n",
    "\n",
    "\n",
    "        # load audio. Using librosa\n",
    "        out = 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/spectro_file/{0}.png'.format(name) #set the output folder for spectrogram\n",
    "        \n",
    "        wav_file = 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/test_dataset_wav/{0}.wav'.format(name)\n",
    "        \n",
    "        y, sr = librosa.load(wav_file, sr=44100)\n",
    "       \n",
    "        window = y[start_sample:start_sample+length_samples]\n",
    "        # convert to PNG\n",
    "        spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\n",
    "       \n",
    "        print('~~ WAV to spectro convertion accomplished ~~')\n",
    "        \n",
    "        \n",
    "        #print spectrogram\n",
    "\n",
    "        img = Image.open('C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/spectro_file/{0}.png'.format(name))\n",
    "        \n",
    "        print(\"\\nSpectrogram :\")\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        #predicting the chord \n",
    "        \n",
    "        my_spectro = 'C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/spectro_file/{0}.png'.format(name)\n",
    "        chord_spect = cv2.imread(my_spectro,cv2.IMREAD_GRAYSCALE)\n",
    "        value = np.array(chord_spect)/255\n",
    "\n",
    "        T_value = torch.from_numpy(value)  # convert numpy.ndarray to Tensor to feed it in the neural network\n",
    "        c_value = T_value.view(-1,1,105,600)\n",
    "        c_value = c_value.float()\n",
    "        c_value = c_value.to(device)\n",
    "\n",
    "        chord_output = C_model(c_value)\n",
    "        \n",
    "        #predicting the key \n",
    "        \n",
    "        key_spectro = cv2.imread(my_spectro)\n",
    "        value = np.array(key_spectro)/255\n",
    "        \n",
    "        T_value = torch.from_numpy(value)  # convert numpy.ndarray to Tensor to feed it in the neural network \n",
    "        k_value = T_value.view(-1,3,105,600)\n",
    "        k_value = k_value.float()\n",
    "        k_value = k_value.to(device)\n",
    "        \n",
    "        key_output = K_model(k_value)\n",
    "        \n",
    "        found=0\n",
    "    elif (song != \"0\"):\n",
    "        print(\"\\n**Song not found please try again**\")\n",
    "        continue;\n",
    "    else:\n",
    "        continue;\n",
    "    \n",
    "    \n",
    "    \n",
    "    _,chord_predict = torch.max(chord_output.data,1)\n",
    "    _,key_predict = torch.max(key_output.data,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (chord_predict == 0) : # predicted minor\n",
    "        print (f\"\\nThe predicted key of {song} is: {keys[key_predict]} minor\\n\")\n",
    "        \n",
    "    else: # predicted major\n",
    "        print (f\"\\nThe predicted key of {song} is: {keys[key_predict]} major\\n\")\n",
    "\n",
    "    \n",
    "    ann_file = open(\"C:/outs/24Annotations.txt\") #open the annotation file to check the correct key of the song \n",
    "    while True:\n",
    "        line = ann_file.readline()\n",
    "        ann_name, key, chord = line.split(' ', 2)\n",
    "        ann_name = ann_name.strip()\n",
    "        key = key.strip()\n",
    "        chord = chord.strip()\n",
    "        \n",
    "        if (ann_name == song):\n",
    "            break;\n",
    "            \n",
    "            \n",
    "    print(f\"The correct answer of {ann_name} is:\\n\\n{key} {chord}\\n\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    \n",
    "    #delete wav and spectro files\n",
    "    os.remove('C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/test_dataset_wav/{0}.wav'.format(name))\n",
    "    os.remove('C:/Users/Michalis Zeakis/Desktop/university/ptyxiaki/datasets/spectro_file/{0}.png'.format(name))\n",
    "    \n",
    "print(\"\\n\\nΕΥΧΑΡΙΣΤΟΥΜΕ ΠΟΛΥ ΓΙΑ ΤΗΝ ΠΡΟΣΟΧΗ ΣΑΣ !!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
